{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04c8c81e",
   "metadata": {},
   "source": [
    "# SS 2021 SEMINAR 05 Reinforcement Learning in der Sprachtechnologie\n",
    "## Reinforcement Learning II "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639ec2fc",
   "metadata": {},
   "source": [
    "## A) Discussion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeeeb7fc",
   "metadata": {},
   "source": [
    "**Interesting papers I stumbled upon recently:**\n",
    "\n",
    "* Planning to Explore with Self-Supervised Models: https://www.semanticscholar.org/paper/Planning-to-Explore-via-Self-Supervised-World-Sekar-Rybkin/3a71c306eb6232658c9e5fd48aed1ef3befe5fbe\n",
    "\n",
    "* World Models: https://www.semanticscholar.org/paper/World-Models-Ha-Schmidhuber/ff332c21562c87cab5891d495b7d0956f2d9228b\n",
    "\n",
    "**RL Applications that could be interesting for you:**\n",
    "\n",
    "* Robotics: Solving Rubik's Cube: https://www.youtube.com/watch?v=x4O8pojMF0w\n",
    "\n",
    "* Robotics: Do you love me?: https://www.youtube.com/watch?v=fn3KWM1kuAw\n",
    "\n",
    "* Self-Driving-Cars: Openpilot: OPEN SOURCE SELF-DRIVING CARS(!!!) https://www.youtube.com/watch?v=YJzvrDBQwOE, https://github.com/commaai/openpilot\n",
    "\n",
    "* Dota 2: OpenAI5 https://www.youtube.com/watch?v=UZHTNBMAfAA\n",
    "\n",
    "**Practical RL Tutorials** \n",
    "\n",
    "* OpenAI Gym + Keras: https://www.youtube.com/watch?v=hCeJeq8U0lo\n",
    " \n",
    "\n",
    "**Presentations** \n",
    "\n",
    "* Please find our dates in our repository: https://github.com/clause-bielefeld/SS_2021_SEMINAR_Reinforcement_Learning_in_der_Sprachtechnologie/blob/main/README.md\n",
    "\n",
    "**REMEMBER: HOMEWORK SUBMISSION DEADLINE: May 19th, MAIL: henrik.voigt@uni-jena.de, NAMING: HOMEWORK_01_FIRSTNAME_LASTNAME.ipynb**\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b81f111",
   "metadata": {},
   "source": [
    "## B) Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e412d48",
   "metadata": {},
   "source": [
    "### REINFORCEMENT LEARNING II\n",
    "\n",
    "#### RL in Detail\n",
    "\n",
    "\n",
    "**Reinforcement Learning is creating a POLICY function that enables an agent to reach a certain GOAL in an optimal way, by using sparse positiv/negative feedback.**\n",
    "\n",
    "**Deep Reinforcement Learning is the application of DEEP LEARNING methods to learn the POLICY function.**\n",
    "\n",
    "<br>\n",
    "\n",
    "**LET'S DRAW WHAT WE KNOW SO FAR**: \n",
    "\n",
    "<img src=\"https://www.wikihow.com/images/0/08/Draw-a-Map-of-an-Imaginary-Place-Step-12.jpg\" width=\"500\"/>\n",
    "\n",
    "<br>\n",
    "\n",
    "What do Reinforcement Learning Agents learn? \n",
    "\n",
    "Optimal Policies!\n",
    "\n",
    "**GOALS** \n",
    "\n",
    "Goals can be formulated in terms of **RETURN**. This makes it very handy because BIG goals can be **deconstructed** into smaller sub-goals, which  are states of the world that give some reward and when solved together the expected return is maximized (and the overall goal is solved). \n",
    "\n",
    "Therefore we define a goal, which can be decomposed into smaller subgoals and for each of those subgoals we get a positive reward. To reach the overall goal in an **OPTIMAL** way, the expected return has to be maximized. \n",
    "\n",
    "<img src=\"https://dsmedia.ign.com/ds/image/article/122/1223480/new-super-mario-bros-2-20120420085937760-001.jpg\" width=\"500\"/>\n",
    "    \n",
    "<br>    \n",
    "<br>     \n",
    "\n",
    "**POLICIES** \n",
    "\n",
    "A policy in a simple way is just a function that takes in a state and puts out an action.\n",
    "\n",
    "Policies are \"super-plans\". A policy is like having the optimal plan, but from  **EVERY STATE** of the world, no matter which. That means, no matter where we start, we know the optimal action that maximizes the expected return. Always. \n",
    "\n",
    "Under the hood a policy is a probability distribution over actions, where the probability of taking action a in state s is PI(a|s). \n",
    "        \n",
    "Optimal Policy: \n",
    "\n",
    "The **OPTIMAL POLICY** is the policy, that yields more return to the agent than all other policies. A policy is better than another policy if the **VALUE** of it (=expected return in every state) is greater or equal than the return of another policy in ALL states. \n",
    "\n",
    "The optimal policy has an associated optimal *state-value function*. The optimal policy has an associated optimal *action-value function*. \n",
    "    \n",
    "<img src=\"https://miro.medium.com/max/386/1*dpJKumwuULjYKLaHjTxr7g.png\" width=\"500\"/>\n",
    "\n",
    "<br>    \n",
    "<br>   \n",
    "\n",
    "**VALUE FUNCTIONS** \n",
    "\n",
    "Value functions are functions of states or state-action pairs. A value function in general estimates how good a certain **STATE** or a certain **ACTION** (EVALUATION of states and actions => estimate how good it is to be in a state concerning the reaching of a goal or how good it is to take a certain action in a state concerning a goal).\n",
    "\n",
    "Value functions are always directly connected to/dependend to a policy!! That means, that having a policy, we can compute the VALUE of each state respected to that policy. \n",
    "\n",
    "How do we derive the VALUE of a state?  \n",
    "\n",
    "We define the value of a state as the **expected, discounted reward** by entering the state and following policy pi towards the goal afterwards. This can be seen as a projection of the future return into the respective state. \n",
    "\n",
    "But how do we derive this then for all states?\n",
    "\n",
    "By using the **BELLMAN EQUATION**.\n",
    "    \n",
    "The *BELLMAN EQUATION* allows us to derive the VALUE of a STATE. It also shows us that the value can be **decomposed** into the **IMMEDIATE REWARD** in that state and the **DISCOUNTED REWARD** of all future states towards the goal following a policy PI. \n",
    "\n",
    "<br>    \n",
    "<br>  \n",
    "\n",
    "<img src=\"https://miro.medium.com/max/379/1*RIMbwQAUJymRMgo5gMJZVg.png\" width=\"500\"/>\n",
    "\n",
    "<br>    \n",
    "<br>  \n",
    "\n",
    "**BELLMAN UPDATE**\n",
    "\n",
    "The Bellman Update tells us how to update our values after each iteration: \n",
    "\n",
    "<img src=\"https://miro.medium.com/max/537/1*W1tVhkVa9obVCvA3PTJ_gw.png\" width=\"500\"/>\n",
    "\n",
    "That means, that the **Optimal Value Function** is recursively related to the Bellman Optimality Equation. \n",
    "\n",
    "STATE-VALUE-FUNCTION:\n",
    "\n",
    "State value functions evaluate each state with respect to its **RETURN** following a policy PI. Which states are the most valueable states? which state from my current one is the most valuable next one? \n",
    "\n",
    "State value function gives us the value of a state S under a policy PI, that means how good is it to be in that state concerning the maximization of our expected return. \n",
    "    \n",
    "<img src=\"https://miro.medium.com/max/700/1*7Kjo-ibNy_jDX2hvnLnROA.png\" width=\"600\"/>\n",
    "\n",
    "<br>    \n",
    "<br>    \n",
    "<br>    \n",
    "<br>    \n",
    "    \n",
    "ACTION-VALUE-FUNCTION:\n",
    "\n",
    "The action-value function is evaluating each **state-action pair** with respect to its **RETURN** following policy PI. Which action is the most valuable one in the current state?\n",
    "\n",
    "The action value function tells us how good it is to take any of the possible actions a in a given state following policy pi. This means it gives us the value of an action in a specific state under policy pi. \n",
    "\n",
    "The value of an action in a certain state is called the Q value. The action value function is sometimes also called Q Function (technically this is depending on the knowledge of a TRANSITION function ...).  \n",
    "\n",
    "The optimal q-function must satisfy the BELLMANN OPTIMALITY EQUATION, which is used to update it. \n",
    "\n",
    "<img src=\"https://miro.medium.com/max/700/1*4Fdbohxs6LedqarJjw65Qg.png\" width=\"500\"/>\n",
    "\n",
    "<br>    \n",
    "       \n",
    "    \n",
    "***\n",
    "\n",
    "\n",
    "### Reinforcement Learning Algorithms    \n",
    "\n",
    "#### Model Based Reinforcement Learning\n",
    "    \n",
    "The idea in model-based-RL is that we can solve a markov decision problem programmatically when having the **STATE TRANSITION FUNCTION** and the **REWARD FUNCTION** completely through computation. \n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Jie-Fu-24/publication/274094617/figure/fig1/AS:614238733217807@1523457334832/Example-of-an-MDP-with-states-Q-s-i-i-0-7-actions-A-a-b-and.png\" width=\"500\"/>\n",
    "\n",
    "The base for this computation are the **BELLMAN EQUATIONS**, which prove, that a markov decision process can be solved when having the STATE TRANSITION FUNCTION and the REWARD FUNCTION by computing an OPTIMAL STATE VALUE FUNCTION. \n",
    "\n",
    "Two methods for solving this are either VALUE ITERATION or POLICY ITERATION. To iteratively build the value function or the policy in each step a BELLMAN UPDATE / BACKUP is performed\n",
    "    \n",
    "Methods applied in this field are *dynamic programming*(DIVIDE & CONQUER) or *monte-carlo-methods*(INTELLIGENT SAMPLING). This method implies the extraction of important **concepts** of the environment (e.g. opponents, positions, states) and the **transitions** (= mechanics/physics of the environment, e.g. is it turn-based, how to move, act, ...). \n",
    "     \n",
    "#### Value Iteration\n",
    "    \n",
    "= Iterating the State-Value Function using Bellman Update Rules (top down). Model based approach, because we know the transition probabilities and rewards. In Q Learning the agent does also a form of value iteration, but it does not know state transition probabilities. \n",
    "<br> \n",
    "<br>\n",
    "#### Policy Iteration\n",
    "\n",
    "= Iterating the Policy (function that maps states to actions). For each policy we derive, we compute the value function. Then we evaluate the value function the policy creates (policy evaluation) and improveme our policy using the bellman update (bottom up).\n",
    "<br>    \n",
    "<br>\n",
    "#### PLANNING | SEARCHING | DYNAMIC PROGRAMMING | MONTE CARLO METHODS | WORLD METHODS\n",
    "    \n",
    "Planning is an adaptive way of model based reinforcement learning, which is based because of the HIGH COMPUTATIONAL EFFORT that value iteration and policy iteration deliver. \n",
    "\n",
    "In value iteration and policy iteration the TRANSITION FUNCTION and the REWARD FUNCTION are fully known.\n",
    "\n",
    "It becomes easy to see, that in more complex set ups with bigger state and action spaces this becomes not computationally feasable to solve for an agent. \n",
    "\n",
    "One method to solve this is to use PLANNING, which is deriving an OPTIMAL PATH from the current state to the GOAL STATE, or, if the state space is too vast (as e.g. in chess, dota, ...) to derive an N-STEP LOOK-AHEAD towards the goal state. \n",
    "<br>\n",
    "<br>\n",
    "#### Model-Free Reinforcement Learning\n",
    "    \n",
    "The idea in model-free RL is to learn how to act optimally based on past **experience**/interactions with the environment and derive an optimal policy from that without building a model of the world, which is basically NOT KNOWING A TRANSITION FUNCTION and NOT KNOWING THE REWARD FUNCTION (INTUITION).  \n",
    "\n",
    "For updating our action-value function we use a derivative of the bellman update for Q-Functions. A Q-Function is basically the same as a action-value function WITHOUT having a TRANSITON FUNCTION. \n",
    "<br>\n",
    "<br>\n",
    "#### Q-Learning\n",
    "\n",
    "Q-Learning is an approach of solving an MDP by LEARNING the optimal q-values. A Q-VALUE is the value of an ACTION in a certain STATE. Q Learning is model free approach. \n",
    "\n",
    "That means, a Q-VALUE describes how VALUABLE it is to execute an ACTION concerning the **expected, discounted reward**, by following the current policy PI. \n",
    "\n",
    "The Q-Learning algorithm updates the q-values of each state action pair using the bellman optimality equation and the bellman update rules. \n",
    "    \n",
    "Exploration Exploitation dilemma approach here: when choosing an action, we can introduce a factor epsilon here, that randomly decides to explore or exploit, which implements how we face this dilemma. \n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59f2ce0",
   "metadata": {},
   "source": [
    "## C) Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896573e0",
   "metadata": {},
   "source": [
    "#### Reinforcement Learning II\n",
    "\n",
    "Today: Implementing our first bot!\n",
    "\n",
    "I am using the gym environment of openai today. \n",
    "You can install it this way: \n",
    "\n",
    "`conda install -c conda-forge gym`\n",
    "\n",
    "The game we try to solve is the frozen lake game:\n",
    "<img src=\"https://camo.githubusercontent.com/f558f268f3c1a45f0a88342113476f34ce894896c30b66fdc3101c8d090a0a0a/68747470733a2f2f616e616c7974696373696e6469616d61672e636f6d2f77702d636f6e74656e742f75706c6f6164732f323031382f30332f46726f7a656e2d4c616b652e706e67\" width=\"500\"/>\n",
    "\n",
    "For this we use the openai gym environment implementation from here: \n",
    "https://gym.openai.com/envs/FrozenLake-v0/\n",
    "\n",
    "You can find the source code in this repository:\n",
    "https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py\n",
    "\n",
    "The environment is an implementation of the same interface as we implemented last week. \n",
    "\n",
    "Let's start:\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "406d0689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Check GPU reachability \n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "74cbe43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym \n",
    "import random \n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15b857af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an instance of the frozen lake gym environment\n",
    "env = gym.make(\"FrozenLake-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "53478b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(16)\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "# get some information from the environment\n",
    "action_space = env.action_space\n",
    "#print(action_space)\n",
    "action_space_size = env.action_space.n\n",
    "#print(action_space_size)\n",
    "state_space = env.observation_space \n",
    "print(state_space)\n",
    "state_space_size = env.observation_space.n\n",
    "print(state_space_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c712b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-TABLE\n",
    "# build our action-value table | Q-TABLE\n",
    "# as you already know, the q-table looks like this\n",
    "# state | action_space\n",
    "\n",
    "q_table = np.zeros((state_space_size, action_space_size))\n",
    "#print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "22aaecc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "num_episodes = 10000\n",
    "max_steps_per_episode = 100\n",
    "\n",
    "# q-learning | update parameters\n",
    "learning_rate = 0.1\n",
    "discount_rate = 0.99\n",
    "\n",
    "# exploration-exploitation trade off\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4ab121f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-LEARNING\n",
    "\n",
    "# collect rewards somewhere to visualize our learning curve\n",
    "rewards_of_all_episodes = []\n",
    "\n",
    "# Training Loop\n",
    "for episode in range(num_episodes):\n",
    "    # reset/initialize the environment first\n",
    "    state = env.reset()\n",
    "    # set done back to false at the beginning of an episode\n",
    "    done = False\n",
    "    # reset our rewards collector | return for the beginning episode\n",
    "    rewards_current_episode = 0\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        # select an action\n",
    "        # use our exploration exploitation trade off -> do we explore or exploit in this timestep ?\n",
    "        exploration_rate_threshold = random.uniform(0,1)\n",
    "        if(exploration_rate_threshold > exploration_rate):\n",
    "            action = np.argmax(q_table[state, : ])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Update Q-Table Q(s,a) using the bellman update  \n",
    "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + learning_rate * (reward + discount_rate * np.max(q_table[new_state, : ]))\n",
    "        \n",
    "        # update the state to the new state\n",
    "        state = new_state\n",
    "        # collect the reward\n",
    "        rewards_current_episode += reward\n",
    "        \n",
    "        if (done == True):\n",
    "            break\n",
    "    \n",
    "    # after we finish an episode, make sure to update the exploration rate\n",
    "    # decay the exploration rate the longer the time goes on\n",
    "    exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "    # append our rewards for this episode for learning curve\n",
    "    rewards_of_all_episodes.append(rewards_current_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "98c09bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****INFO: average reward per thousand episodes: ***** \n",
      "\n",
      "1000 :  0.037000000000000026\n",
      "2000 :  0.19900000000000015\n",
      "3000 :  0.3960000000000003\n",
      "4000 :  0.5760000000000004\n",
      "5000 :  0.6420000000000005\n",
      "6000 :  0.6700000000000005\n",
      "7000 :  0.6820000000000005\n",
      "8000 :  0.6660000000000005\n",
      "9000 :  0.6840000000000005\n",
      "10000 :  0.6730000000000005\n",
      "\n",
      "\n",
      " ***** Q-TABLE ***** \n",
      "\n",
      "[[0.59709926 0.45967051 0.49482058 0.49042446]\n",
      " [0.27493479 0.35435928 0.33467266 0.50606189]\n",
      " [0.38700126 0.37887474 0.37949189 0.45597048]\n",
      " [0.29731293 0.3101768  0.30679792 0.42787359]\n",
      " [0.61297439 0.35201328 0.43065625 0.38352635]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.17116911 0.17446989 0.36932612 0.13972523]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.50810251 0.40024022 0.43867213 0.64934984]\n",
      " [0.46785004 0.70719625 0.34680311 0.49487922]\n",
      " [0.61295413 0.40262048 0.37166018 0.3131942 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.54603719 0.56039832 0.77424079 0.46169362]\n",
      " [0.71762835 0.8768891  0.77447327 0.75924037]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# LEARNING STATISTICS \n",
    "# for each episode print the stats of the episode\n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_of_all_episodes),num_episodes/1000)\n",
    "count = 1000\n",
    "print('*****INFO: average reward per thousand episodes: ***** \\n')\n",
    "for reward in rewards_per_thousand_episodes:\n",
    "    print(count, \": \", str(sum(reward/1000)))\n",
    "    count += 1000\n",
    "    \n",
    "# print our learned q-table\n",
    "print(\"\\n\\n ***** Q-TABLE ***** \\n\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5efd9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "INFO: ***** agent reached the goal. *****\n"
     ]
    }
   ],
   "source": [
    "# PURE EXPLOITATION | watching our agent play\n",
    "for episode in range(3):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print(\"INFO:*****EPISODE \", episode+1, \"\\n\\n\\n\")\n",
    "    time.sleep(1)\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.3)\n",
    "        \n",
    "        action = np.argmax(q_table[state, :])\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if reward == 1:\n",
    "                print(\"INFO: ***** agent reached the goal. *****\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"INFO: ***** agent died.\")\n",
    "                time.sleep(3)\n",
    "            clear_output(wait=True)\n",
    "            break\n",
    "        \n",
    "        state = new_state\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8d10af9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEEP REINFORCEMENT LEARNING\n",
    "# DEEP Q LEARNING\n",
    "\n",
    "# next week"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848d671d",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c888b9",
   "metadata": {},
   "source": [
    "# TODO's\n",
    "\n",
    "1. Send your finished presentations (+ possibly annotated paper) by **Monday 12.00 AM/midnight** via email to henrik.voigt@uni-jena.de\n",
    "\n",
    "3. SEND your little HOMEWORK to henrik.voigt@uni-jena.de unitl **19TH OF MAY** by using the naming convention: HOMEWORK_01_FIRSTNAME_LASTNAME.ipynb until May 19th 00.00 AM/midnight\n",
    "\n",
    "***\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env-kernel",
   "language": "python",
   "name": "pytorch-env-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
