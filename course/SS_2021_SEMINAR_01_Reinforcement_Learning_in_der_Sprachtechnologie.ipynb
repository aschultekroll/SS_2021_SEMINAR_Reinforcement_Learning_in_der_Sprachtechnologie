{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "passive-congress",
   "metadata": {},
   "source": [
    "# SS 2021 SEMINAR 01 Reinforcement Learning in der Sprachtechnologie\n",
    "## Einführungsveranstaltung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entitled-angola",
   "metadata": {},
   "source": [
    "## A) Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-trading",
   "metadata": {},
   "source": [
    "### Title: A Survey on Reinforcement Learning for Dialogue Systems\n",
    "\n",
    "##### Link: \n",
    "[A Survey on Reinforcement Learning for Dialogue Systems](https://www.semanticscholar.org/paper/A-Survey-on-Reinforcement-Learning-for-Dialogue-Gra%C3%9Fl/cafc84a0b328f40af05fbc857621b3441316688f)\n",
    "\n",
    "##### Summary:\n",
    "Das Paper umreißt den aktuellen State-of-the-Art bezüglich Methoden und Algorithmen des Reinforcement Learning, welche in Dialogsystemen eingesetzt werden. Die Autoren zeigen auf, wie RL in DS eingesetzt werden kann, diskutieren aktuelle Lösungen, zukünftige Ideen und Probleme und Grenzen des Ansatzes. Das Paper geht zunächst darauf ein, was RL ist, wie diese Prinzipen auf ein Dialog-Setting angewandt werden und schlägt anschließend eine Kategorisierung von Dialogsystemen bezüglich der Domänspezifität und der Antwortkomplexität vor. \n",
    "\n",
    "##### Problem/Task/Question:\n",
    "Die Aufgabe, die das Paper beleuchtet ist die Schaffung eines Überblicks über aktuelle Forschungsarbeiten im Bereich Dialogsysteme, welche mit Reinforcement Learning arbeiten. \n",
    "\n",
    "##### Solution/Idea/Model/Approach:\n",
    "![alt text](https://d3i71xaburhd42.cloudfront.net/cafc84a0b328f40af05fbc857621b3441316688f/3-Figure3-1.png \"\")\n",
    "\n",
    "* **RL im Allgemeinen: System lernt was zu tun ist/wie zu handeln ist/VERHALTEN.**\n",
    "* -- Agent\n",
    "* -- Environment\n",
    "* -- Menge von States S = STATE SPACE\n",
    "* -- Menge von Actions A = ACTION SPACE\n",
    "* -- Feedbackfunktion/Reward Function (= Funktion, die das ZIEL vorgibt oder definiert was GUT und was SCHLECHT ist)\n",
    "* -- Bewertungsfunktion für State-Action Paare (= Funktion, die jedes state-action paar bezüglich des langfristigen returns bewertet)\n",
    "* -- Policy = Funktion die für einen State als Eingabe eine Action als Ausgabe liefert, HAUPTZIEL von RL. \n",
    "* -- Exploration-Exploitation Balance\n",
    "\n",
    "* **Dialogsysteme im Allgemeinen: Systeme mit Sprachein- und/oder Ausgabe**\n",
    "* Typen: gesprochene Sprache, geschriebene Sprache, Multi-Modale Systeme (z.b. Sprache und Bild/Video), Zielorientiert oder Offen\n",
    "* -- Input Modul: (Text, Sprache, Audio, ...) => Text wahrnehmen und festhalten\n",
    "* -- NLU (Natural Language Understanding) Modul: Text verstehen = ZIEL des Nutzers verstehen\n",
    "* -- Dialogue Manager: bekommt als eingabe das verstandene ZIEL und wählt eine Action aus. Besteht aus WORLD MODEL/State tracker + POLICY + MEMORY(dialogue history + knowledge sources)\n",
    "* -- NLG (Natural Language Generation) Modul => Text ausgeben (Sprache, Chat, ...)\n",
    "\n",
    "* **INTEGRATION von RL in DS:** \n",
    "* -- Feedback/Reward Function => use positive or negative signals from the user (- i do not understand, + thank you), Mult-Modal -> sentiments, face expressions. Unterschiedliche Feedback-Funktionen je nach Ziel des Systems (Nutzerzufriedenheit, Emotionale Verfassung des Nutzers, Wie schnell wurde das Ziel tatsächlich erreicht, ..)\n",
    "* -- Lernen auf Basis von Chat-Verläufen/Sequenzen => Leite daraus eine POLICY ab\n",
    "* -- States = Input Queries (Texteingaben) -> CURRENT STATE, Chat History = Memory\n",
    "* -- Actions = Database Queries + Übersetzung der Ausgabe in natürliche Sprache\n",
    "* -- ZIEL: kreiere mithilfe von RL Systemantworten, die kontextsensitiv und semantisch geeignet sind um den Nutzer optimal zu seinem ZIEL zu führen. \n",
    "\n",
    "\n",
    "##### 3 Main Results/Findings: \n",
    "* **DS Typen**: Chatbots, Infobots, Goal-oriented Bots/Task-Completion Bots, Personal Assistants\n",
    "* **Underlying DS Design Principles:** Grammatik/Regel-basierte Systeme, Probabilistische Systeme, Neuronale Systeme\n",
    "\n",
    "* **Um einen Überblick zu gewinnen, schlagen die Autoren folgende Taxonomy vor:**\n",
    "\n",
    "* a) Closed-Domain and Retrieval Based: Aufgabe ist klar und Domäne ist bekannt. Z.B. Lege eine Flugreservierung an, Reserviere einen Termin in meinem Kalender. Framing: Finde mithilfe des Dialoges alle entscheidenden VARIABLEN heraus (=Collection, Slot-Filling) und rufe dann eine FUNKTION auf und gib deren Ergebnis in natürlicher Sprache zurück. Lösungen: Phrase Matching, Semantic Parsing, Statistische Methoden. Beispiel: ELIZA (psychologisch motivierter Chatbot). \n",
    "\n",
    "* b) Closed Domain and Generative Based: Aufgabe und Domain sind bekannt, es wird aber kein Slot-Filling betrieben, sondern versucht eine LÖSUNG zu finden und diese in natürlicher Sprache zu kommunizieren. Supervised Learning (auf Basis von Beispiellösungen) in Kombination mit Reinforcemen Learning (zum Ranking der Güte der Lösung) wird eingesetzt. Beispiel: IBM Watson: Der Nutzer hat eine konkrete Frage zu einer Domäne, wie etwa `Was ist der beste Weg, Krankheit XYZ zu behandeln?` und basierend auf den Daten berechnet der Agent eine Lösung/Lösungsweg und kommuniziert diesen. \n",
    "\n",
    "* c) Transfer Learning and Opem Domain DS: Generalisiere Aufgaben über Domänen hinweg. Input und Output sind nicht limiert auf eine einzige Domäne oder Aufgabe. Z.B. Buche Flüge, Restaurants, Hotels, Termine, .. => Der BOT versteht das Konzept `BUCHUNG` und kann es domänenunabhängig anwenden. Beispiel: Microsoft Xiaoice. Dieses Gebiet ist Teil aktueller Forschung. \n",
    "\n",
    "* **Die Autoren sehen folgende Grenzen/Probleme:**\n",
    "* -- mangelndes semantisches Verständnis der Bots\n",
    "* -- mangelnde Robustheit für diverse Eingaben (Dialekte, Typing Errors)\n",
    "* -- Schwierigkeit, exakte und geeignete Reward-Functions zu definieren (Was ist eine positive Antwort? Wann ist eine Antwort positiv? Wie positiv ist eine Antwort?)\n",
    "\n",
    "##### Critical Discussion: \n",
    "\n",
    "* **+** sehr logische Struktur des Papers: Allgemeine RL -> RL angewandt/integriert in DS -> eigener Lösungsvorschlag einer Taxonomy\n",
    "* **+** gute Illustrationen des Aufbaus eine DS mit RL Integration\n",
    "* **+** gute Ableitung von Problemen und (aktuellen) Grenzen des Ansatzes\n",
    "\n",
    "* **-** RL ist (meiner Meinung nach) keine Methode, die dem 'natürlichen' Lernverhalten von Menschen nachempfunden ist, sondern (meiner Meinung nach) der Versuch, allgemein zu definieren, wie optimale Verhaltensweisen/Policies/Strategien anhand von Daten automatisch gelernt werden können. \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-inspection",
   "metadata": {},
   "source": [
    "## B) Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-liechtenstein",
   "metadata": {},
   "source": [
    "### Moduldetails\n",
    "\n",
    "* Termin: jeden **Mittwoch 12-14 Uhr**, erster Termin: 14.04.2021, letzter Termin: 14.07.2021 \n",
    "* Ort: https://uni-jena-de.zoom.us/j/68193002653\n",
    "* Kennwort: **siehe private Mail**\n",
    "* Leistungspunkte: **3LP**\n",
    "* Prüfungsleistung: einmalige **15-minütige Paper-Präsentation inkl. Ausarbeitung** + 4 Hausaufgaben\n",
    "* Sprache: deutsch (english on demand)\n",
    "* Modulverantwortliche: **Prof. Sina Zarrieß** und **Henrik Voigt**\n",
    "\n",
    "### Inhalt \n",
    "\n",
    "* Short Recap: Machine Learning\n",
    "* Short Recap: Implementierung von Machine Learning Konzepten in Python\n",
    "* Theory: Konzepte des Reinforcement Learning \n",
    "* Theory: Sprachagenten\n",
    "* -- skriptbasierten Agenten\n",
    "* -- Supervised-Learning basierte Agenten\n",
    "* -- Reinforcement Learning basierte Agenten\n",
    "* Practice: Praktische Umsetzung von Reinforcement Learning Konzepten in Python\n",
    "* Practice: Implementierung von Sprachagenten in Python\n",
    "\n",
    "### Ablaufplan\n",
    "\n",
    "* 14.04.2021: Einführungsveranstaltung \n",
    "* 21.04.2021: Short Recap: Konzepte des Machine Learning \n",
    "* 28.04.2021: Short Recap: Implementierung von Machine Learning Konzepten in Python\n",
    "* 05.05.2021: Theory: Grundlagen des Reinforcement Learning I\n",
    "* 12.05.2021: Theory: Grundlagen des Reinforcement Learning II\n",
    "* 19.05.2021: Theory: Sprachagenten\n",
    "* 26.05.2021: Theory: Skriptbasierte Sprachagenten I\n",
    "* 02.06.2021: Theory: Skriptbasierte Sprachagenten II\n",
    "* 09.06.2021: Theory: Supervised Learning basierte Sprachagenten I\n",
    "* 16.06.2021: Theory: Supervised Learning basierte Sprachagenten II\n",
    "* 23.06.2021: Theory: Reinforcement Learning basierte Sprachagenten I\n",
    "* 30.06.2021: Theory: Reinforcement Learning basierte Sprachagenten II\n",
    "* 07.07.2021: Theory: Reinforcement Learning basierte Sprachagenten III\n",
    "* 14.07.2021: Theory: Code Review, Debugging, Fragen und Probleme\n",
    "\n",
    "### Ablauf einer Seminar-Veranstaltung\n",
    "\n",
    "Eine einzelne Seminar-Veranstaltung besteht aus **drei** Teilen: \n",
    "\n",
    "A) **Discussion:** 30 Minuten **Präsentation** und **Diskussion** aktueller Paper aus dem Bereich **Machine Learning**, **Natural Language Processing** (NLP) und **Reinforcement Learning** (RL)\n",
    "\n",
    "B) **Theory:** 30 Minuten **Theory Walkthrough** durch Konzepte des Reinforcement Learning in der Sprachtechnologie\n",
    "\n",
    "C) **Practice:** 30 Minuten **Live Coding** und Implementierung der besprochenen Inhalte in Python mittels Jupyter Notebooks.\n",
    "\n",
    "**WICHTIG:** Sowohl für die Präsentation der Paper als auch für Theorie und Coding werden die Programmiersprache **Python** in Kombination mit **Jupyter Notebooks** verwendet. \n",
    "\n",
    "**WICHTIG:** Paper Präsentationen bitte bis **Montag 12.00 AM/midnight** an **henrik.voigt@uni-jena.de** senden, weil dann alle Teilnehmer rechtzeitig den Code für die folgende Veranstaltung erhalten. \n",
    "* **Template**: Nutzt am besten das Jupyter Notebook der ersten Veranstaltung als Vorlage und ladet dieses kurze Notebook unter folgendem Namen in den Ordner /presentations hoch:\n",
    "* **Filename**: DATUM_PRESENTER z.b. 2021_04_21_Henrik_Voigt.ipynb  \n",
    "* **Optional**: Sendet mir euer annotiertes Paper ebenfalls per Mail zu (Infos folgen), Filename: DATUM_PRESENTER_PAPERNAME_annotated.pdf\n",
    "\n",
    "###  Materialien \n",
    "**WICHTIG:** Wir nutzen für dieses Modul **EIN** zentrales Repository für die Verwaltung aller Kursaufzeichnungen/Notebooks, Termine und Materialen, das heißt **DIESES** Repository ist der einzige Link, den ihr euch für die Veranstaltung merken/speichern müsst. \n",
    "\n",
    "* [Kursrepository](https://github.com/clause-bielefeld/SS_2021_SEMINAR_Reinforcement_Learning_in_der_Sprachtechnologie)\n",
    "\n",
    "* Alternativ: Der Kursordner wird auf unserer Website (https://clause-bielefeld.github.io/teaching) unter /teaching veröffentlicht, sodass ihr ihn da herunterladen könnt\n",
    "\n",
    "\n",
    "* **Jupyter Notebooks**: werden nach jeder Veranstaltung im Kursrepository im Ordner /course veröffentlicht\n",
    "\n",
    "* **Annotierte Paper**: werden nach jeder Veranstaltung im Kursrepository im Ordner /materials veröffentlicht\n",
    "\n",
    "* **Literaturrecherche**: \n",
    "* -- https://www.semanticscholar.org/ **!!!**\n",
    "* -- http://www.arxiv-sanity.com/\n",
    "* -- https://scholar.google.de/\n",
    "* -- https://www.paperswithcode.com\n",
    "* -- https://www.aclweb.org/anthology/\n",
    "\n",
    "* **Themeninspiration**: \n",
    "* -- Yannic Kilcher (https://www.youtube.com/channel/UCZHmQk67mSJgfCCTn7xBfew)\n",
    "* -- Henri AI Labs (https://www.youtube.com/channel/UCHB9VepY6kYvZjj0Bgxnpbw)\n",
    "* -- Two Minute Papers (https://www.youtube.com/user/keeroyz)\n",
    "\n",
    "* **Suggestions:**\n",
    "Som ideas for interesting papers to discuss ...\n",
    "* [Overview Survey Paper](https://www.semanticscholar.org/paper/Reinforcement-Learning%3A-A-Survey-Kaelbling-Littman/12d1d070a53d4084d88a77b8b143bad51c40c38f)\n",
    "* [Introduction Paper](https://www.semanticscholar.org/paper/Reinforcement-Learning%3A-An-Introduction-Sutton-Barto/97efafdb4a3942ab3efba53ded7413199f79c054)\n",
    "* [Tutorial Paper](https://www.semanticscholar.org/paper/Reinforcement-Learning%3A-A-Tutorial.-Harmon-Harmon/e7ac195959e2ce078902b000fc16ef2096fcec10)\n",
    "* [Algorithms for RL Paper](https://www.semanticscholar.org/paper/Algorithms-for-Reinforcement-Learning-Szepesvari/e60f3c1cb857daa3233f2c5b17b6f111ff86698c)\n",
    "* [Deep RL Paper](https://www.semanticscholar.org/paper/Deep-reinforcement-learning%3A-a-survey-Wang-Liu/16e83f3f0f78ceb203746eeb88f1f5aae9ba3092)\n",
    "* [RL and NLP Paper](https://www.semanticscholar.org/paper/A-Survey-of-Reinforcement-Learning-Informed-by-Luketina-Nardelli/7dc156eb9d84ae8fd521ecac5ccc5b5426a42b50)\n",
    "* [RL and NLP Paper II](https://www.semanticscholar.org/paper/An-Overview-of-Natural-Language-State-for-Learning-Madureira-Schlangen/a4069dd677b205fba61b4dea75e26c148dee99c5)\n",
    "\n",
    "\n",
    "### Fragen\n",
    "* **WICHTIG:** immer direkt per **AUDIO** im Zoom Call Fragen stellen! Diskussionen sind ausdrücklich erwünscht!\n",
    "* **ALTERNATIV:** Fragen im **Chat** posten \n",
    "\n",
    "### Umfragen\n",
    "\n",
    "##### Genereller Background\n",
    "Bachelor - Master ? []\n",
    "Studienfach ? []\n",
    "\n",
    "##### Programmierung \n",
    "Skala von 1-10: []\n",
    "\n",
    "##### Python \n",
    "Skala von 1-10: []\n",
    "\n",
    "##### Jupyter Notebooks \n",
    "Skala von 1-10: []\n",
    "\n",
    "##### Machine Learning \n",
    "Skala von 1-10: []\n",
    "\n",
    "##### Natural Language Processing \n",
    "Skala von 1-10: []\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-dictionary",
   "metadata": {},
   "source": [
    "## C) Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sacred-legislation",
   "metadata": {},
   "source": [
    "##### Python Setup\n",
    "\n",
    "* **Anaconda** \n",
    "* -- Installation via https://www.anaconda.com/products/individual due to your system\n",
    "* **Unix:**` conda --version `\n",
    "* **Windows:**\n",
    "1. Navigate to the anaconda directory.\n",
    "2. ![img](https://docs.anaconda.com/_images/win-anaconda-prompt2.png)\n",
    "3. Open your Anaconda Prompt \n",
    "4. Type unix conda commands here ...\n",
    "5. `conda --version`\n",
    "* -- PROBLEM: make sure to add conda to **ENV** variables (https://www.geeksforgeeks.org/how-to-setup-anaconda-path-to-environment-variable/#:~:text=Select%20the%20%E2%80%9CPath%E2%80%9D%20variable%20and,path%20where%20Anaconda%20is%20installed.)\n",
    "***\n",
    "\n",
    "* -- **Environments**\n",
    "* -- create Environment: `conda create -n your_environment_name`\n",
    "* -- see list of conda environments: `conda env list`\n",
    "* -- activate your environment: `conda activate your_environment_name`\n",
    "***\n",
    "\n",
    "* **Packages**\n",
    "* -- **Package Manager:** conda\n",
    "* -- `conda install [package]`\n",
    "* -- Installation via **conda install** vs. conda-forge install vs. pip install\n",
    "* -- anaconda contains 1500 scientific packages (numpy, pandas, scikit, ...) so no need to reinstall them ...\n",
    "* -- external packages: **pytorch**, will be announced in time\n",
    "\n",
    "***\n",
    "* **Jupyter Notebooks**\n",
    "* -- Installation: `conda install -c anaconda jupyter`\n",
    "* -- Start: `jupyter notebook` \n",
    "* **OR MY RECOMMENDATION: Jupyter Lab**\n",
    "* -- Installation:`conda install -c conda-forge jupyterlab`\n",
    "* -- Start: `jupyter lab` or `jupyter-lab`\n",
    "* -- adding conda environments to jupyter lab environments: https://stackoverflow.com/questions/53004311/how-to-add-conda-environment-to-jupyter-lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limited-baseline",
   "metadata": {},
   "source": [
    "# TODO's\n",
    "\n",
    "1. Wählt einen Termin für eure Paper Diskussion/Präsentation bis **Freitag, 30.04.2021 12.00 PM/noon** aus unter https://terminplaner4.dfn.de/XP1MLFhVyz1eA44a\n",
    "2. Sendet eure fertigen Präsentationen (+ evtl. annotated Paper) bis **Montag 12.00 AM/midnight** per Mail an henrik.voigt@uni-jena.de\n",
    "\n",
    "***\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env-kernel",
   "language": "python",
   "name": "pytorch-env-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
