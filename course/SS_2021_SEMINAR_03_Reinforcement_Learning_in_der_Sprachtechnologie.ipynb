{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecec0d28",
   "metadata": {},
   "source": [
    "# SS 2021 SEMINAR 03 Reinforcement Learning in der Sprachtechnologie\n",
    "## Short Recap II: Machine Learning in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5e1308",
   "metadata": {},
   "source": [
    "## A) Discussion\n",
    "\n",
    "TASK FOR YOU: Note **ONE** thing about the paper/topic that we should remember, which can either be:\n",
    "\n",
    "* a **QUESTION** that the presenter should answer you after the presentation\n",
    "\n",
    "* an **ASPECT** that you find especially **INTERESTING** in the paper/topic\n",
    "\n",
    "* a **WORD** that **SUMMARIZES** the paper/topic for you \n",
    "\n",
    "* an **ASSOCIATION/MEMORY** that came into your mind when listening\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98913cfa",
   "metadata": {},
   "source": [
    "### Title: My Title\n",
    "##### Link: \n",
    "[MyPaper]()\n",
    "\n",
    "##### Summary:\n",
    "\n",
    "\n",
    "##### Problem/Task/Question:\n",
    "\n",
    "\n",
    "##### Solution/Idea/Model/Approach:\n",
    "![model_image]()\n",
    "\n",
    "##### 3 Main Results/Findings: \n",
    "\n",
    "##### Critique: \n",
    "\n",
    "\n",
    "|         | **+**           | **-**  |\n",
    "| ------------- |:-------------:|:-----:|\n",
    "\n",
    "***\n",
    "##### Discussion:\n",
    "\n",
    "**THE LEARNING ZOO**\n",
    "\n",
    "(You can find the visualization of this discussion in the /materials folder) \n",
    "\n",
    "**Classification of ML Areas**:\n",
    "\n",
    "How can we classify areas of machine learning ?\n",
    "\n",
    "* General Learning Setup\n",
    "\n",
    "* my personal Tree/Graph Classification/VIEW (DISCUSSION highly appreciated!)\n",
    "\n",
    "* your ideas/feel free to ADD open terms/questions!\n",
    "\n",
    "Unsupervised Learning \n",
    "\n",
    "Supervised Learning\n",
    "\n",
    "Active Learning\n",
    "\n",
    "Semi-Supervised Learning\n",
    "\n",
    "Self-Supervised Learning\n",
    "\n",
    "Reinforcement Learning\n",
    "\n",
    "Contrastive Learning\n",
    "\n",
    "Curriculum Learning\n",
    "\n",
    "Online Learning\n",
    "\n",
    "Offline Learning\n",
    "\n",
    "Meta Learning\n",
    "\n",
    "Transfer Learning\n",
    "\n",
    "Multi Task Learning\n",
    "\n",
    "Evolutionary Learning/Population Based Methods \n",
    "\n",
    "Neural Architecture Search/Model Learning\n",
    "\n",
    "Model Selection\n",
    "\n",
    "Ensemble Learning\n",
    "\n",
    "Swarm Intelligence\n",
    "\n",
    "---\n",
    "\n",
    "**Reward Function**: \n",
    "\n",
    "Why is it important to have a reward function?\n",
    "* BECAUSE it tells you, when you reached your GOAL!\n",
    "\n",
    "\n",
    "How is it structued? \n",
    "> +1 for reaching the goal, -1 for failing, +-0/neutral for any other step\n",
    "\n",
    "\n",
    "<img src=\"https://www.kdnuggets.com/images/reinforcement-learning-fig1-700.jpg\" width=\"500\"/>\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Peter-Holowka/publication/348976339/figure/fig3/AS:994064300257281@1614014806119/An-example-of-a-successful-AWS-DeepRacer-model-reward-function-written-in-Python.ppm\" width=\"500\"/>\n",
    "\n",
    "REWARD FUNCTIONS: \n",
    "\n",
    "<img src=\"data:image/png;base64,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\" width=\"500\"/>\n",
    "\n",
    "\n",
    "These engineered **reward functions** are basically **extrinsic rewards**. If you want to find an analogy to humans, you can think of it like finding **food, water or basic physical needs that prevent harm**, which the agent is striving for. Why is this maybe not the best way to give reward?\n",
    "\n",
    "* sparse rewards problem\n",
    "\n",
    "* discount factors\n",
    "\n",
    "* intrinsic motivation/intrinsic rewards? (exploration vs. exploitation dilemma)\n",
    "\n",
    "\n",
    "DISTANCE FUNCTIONS:\n",
    "\n",
    "A distance function tells us how far things are away from each other.\n",
    "\n",
    "PROBLEM: A distance function is a metric that is \n",
    "defined upon a **SPACE** (e.g. EUCLIDEAN SPACE, SPHERE, ...). \n",
    "\n",
    "The assumption what we currently do most of the time is that the spaces we operate in are euclidean spaces, \n",
    "which might not be true to all learning scenarios... .\n",
    "\n",
    "This encourages to investigate the question if it is possible to learn a DISTANCE FUNCTION FROM DATA, which is \n",
    "a hard task, because it basically flips the setup! (At the moment we learn a model from data BY OPTIMIZING TOWARDS AN OPTIMIMUM/MINIMUM, which is defined by a LOSS FUNCTION(=distance function). The idea is then to use this distance function to let the agent evaluate himself on the task. (RL -> self-supervised learning). \n",
    "\n",
    "Euclidean Distance: \n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/800/1*ZrwEraj9S-u_KOWdKWc8sQ.png\" width=\"500\"/>\n",
    "\n",
    "Spherical Distance: \n",
    "\n",
    "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRC3T9BnG1rUDd2KlL57dUG3XTHPlqhBzYirjFKQrwXHcM29NXVgeaJXN0Iktg9OYKU7ts&usqp=CAU\" width=\"500\"/>\n",
    "\n",
    "\n",
    "There is tons of other distance functions that have been proposed: COSINE DISTANCE, MANHATTAN DISTANCE, MAHALANOBIS DISTANCE,...\n",
    "\n",
    "My current concerns are: \n",
    "\n",
    "*Why are we so convinced about the fact that our data spaces are in fact euclidean? I mean since Einstein we know that space is curved ....*\n",
    "\n",
    "*Why are we so shure that the reward is given by the environment? Should we split reward functions into extrinsic and intrinsic reward?*\n",
    "\n",
    "**What are your thoughts on that?**\n",
    "\n",
    "---\n",
    "\n",
    "**Evaluation**: \n",
    "How do we evaluate Speech Agents (Turing, Chinese Room, ...)\n",
    "\n",
    "* Accepted METRICS (LOSS, ACCURACY, BLEU, CIDER, ROUGE, TIME, FLOPS, ...)\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Benoit-Gallix/publication/324457640/figure/fig1/AS:622298201595905@1525378861825/Graph-illustrating-the-impact-of-data-available-on-performance-of-traditional-machine.png\" width=\"500\"/>\n",
    "\n",
    "* Benchmark Evaluation (e.g. in Computer Vision -> ImageNet, CIFAR10, MSCOCO or in NLP Wikicorpus, SQuAD)\n",
    "\n",
    "<img src=\"https://i0.wp.com/syncedreview.com/wp-content/uploads/2020/06/Imagenet.jpg?fit=1400%2C600&ssl=1\" width=\"500\"/>\n",
    "\n",
    "* Human Evaluation (Qualitative = Questionnaire, Quantitative = Dialogue Techniques, free)\n",
    "\n",
    "<img src=\"https://www.bachelorprint.de/wp-content/uploads/2018/10/Fragebogen-Erstellen-1.jpg\" width=\"500\"/>\n",
    "\n",
    "* Combined Methods (e.g. in VIS Human Eval + Benchmark)\n",
    "\n",
    "* Measure of Intelligence\n",
    "\n",
    "Chollet suggests a general **measure of intelligence**, which is described as the **speed of skill aquisition** by having as less experience (=data) as possible. \n",
    "\n",
    "That means when comparing two systems that saw the same amount of data, one system is more intelligent, if it aquires the skill (e.g. image classification) in a shorter time OR if at a certain point in time it reaches the same skill level by seing less data. \n",
    "\n",
    "<img src=\"https://i.ytimg.com/vi/mEVnu-KZjq4/maxresdefault.jpg\" width=\"500\"/>\n",
    "\n",
    "GENERAL SETUP:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/3472/1*Eqgy6p9nHyeRkpzU-02-Mw.png\" width=\"500\"/>\n",
    "\n",
    "BENCHMARK DATASET: \n",
    "\n",
    "<img src=\"https://pbs.twimg.com/media/EItEQyDWwAIcNWI.png:large\" width=\"500\"/>\n",
    "\n",
    "**Philosophical Question:** \n",
    "Some thoughts on my philosophical question ...\n",
    "\n",
    "Why do we not use learning curves for humans? Is this actually a hint for RL? (infinite tensorspaces?)\n",
    "* Infinite Datasets\n",
    "* Unclear Definitions of what e.g. is Machine Learning -> how could we claim to score someone about that\n",
    "* \n",
    "\n",
    "* BUT would it be something nice to have/motivating ? \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd5fa04",
   "metadata": {},
   "source": [
    "## B) Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0271edd2",
   "metadata": {},
   "source": [
    "### FUNDAMENTALS OF MACHINE LEARNING II\n",
    "#### DATA in NLP \n",
    "\n",
    "* Corpora\n",
    "\n",
    "A corpus in general is a list of **documents**, where each document is a list of **tokens**. \n",
    "\n",
    "Some important concepts:\n",
    "* Tokenization: Today is a great day -> 'today', 'is', ...\n",
    "* Stemming: trouble, troubles, troublesome -> troubl \n",
    "* Lemmatization: going -> go\n",
    "* Vocabulary: Set of all different tokens of a corpus\n",
    "* TF-IDF: term-frequency-inverse-document-frequency -> relative importance weighting of a single tokens occurence in a single document to its occurance in the whole corpus\n",
    "* Language Models: A Sequence Model that given an input sequence is able to predict the next token of the sequence. \n",
    "\n",
    "<img src=\"data:image/png;base64,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\" width=\"500\"/>\n",
    "\n",
    "<img src=\"https://i.ytimg.com/vi/pMv-rxqBhsY/maxresdefault.jpg\" width=\"500\"/>\n",
    "\n",
    "* Famous corpora\n",
    "\n",
    "PENN TREEBANK\n",
    "\n",
    "<img src=\"https://paperswithcode.com/media/datasets/treebank.png\" width=\"500\"/>\n",
    "\n",
    "WIKI CORPUS\n",
    "\n",
    "<img src=\"https://admin.itsnicethat.com/images/GPg6nk_f4NhfUM-3-Ypm4MrwzDk=/193385/format-webp%7Cwidth-2880/wikipedia-redesign-ux-digital-itsnicethat_917px-04_-_DIP_-_Language_links_.jpg\" width=\"500\"/>\n",
    "\n",
    "MSCOCO\n",
    "\n",
    "<img src=\"https://www.mdpi.com/applsci/applsci-08-00909/article_deploy/html/images/applsci-08-00909-g007.png\" width=\"500\"/>\n",
    "\n",
    "Multimodal Corpora like Visual Genome, Spoken Text Corpora, Machine Translation Corpora, ...\n",
    "\n",
    "```\n",
    "QUESTION TO YOU: Did anyone work with text corpora already, if yes type your corpus here (does anyone not know, how a corpus looks like)?  \n",
    "\n",
    "A: yes, wikicorpus\n",
    "\n",
    "B: no\n",
    "```\n",
    "\n",
    "#### MODELS in NLP \n",
    "\n",
    "* Embeddings: lower dimensional representations of word tokens for faster computation + semantic conserving representation\n",
    "\n",
    "<img src=\"https://hackernoon.com/images/yop32ir.jpg\" width=\"500\"/>\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*xD9n3KeWXuenMNL_BpYp6A.png\" width=\"500\"/>\n",
    "\n",
    "How do we train our embedding models? \n",
    "\n",
    "> Word2Vec (Skip-Gram, CBOW-Method)\n",
    "\n",
    "<img src=\"data:image/png;base64,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\" width=\"500\"/>\n",
    "\n",
    "The **weight** the neural network learns is our **lookup table** for our word embeddings. \n",
    "\n",
    "> BERT EMBEDDING (Contextualized Embeddings)\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/600/1*IpzhNX2R1GagUNDidgnNiQ.png\" width=\"500\"/>\n",
    "\n",
    "Contextualized embeddings allow us to use context dependend representations of tokens that capture the meaning more granular and allow to deal with **disambiguation**. \n",
    "\n",
    "\n",
    "**Classification in NLP**\n",
    "\n",
    "Classifiers in NLP are related to the following tasks:\n",
    "\n",
    "> sentiment analysis/classification\n",
    "\n",
    "> spam detection\n",
    "\n",
    "> hate speech detection\n",
    "\n",
    "> ...\n",
    "\n",
    "The idea is to train a classifier to detect patterns in the data that allow **text classification** based on the extracted features learned from text only. \n",
    "\n",
    "\n",
    "**Regression in NLP**\n",
    "\n",
    "Regression in NLP often happens in the form of **auto-regressive models** which are models that **take their own output as input** to produce the next word/sentence/utterance. \n",
    "\n",
    "<img src=\"https://www.kdnuggets.com/wp-content/uploads/reccurrent-network-arch.png\" width=\"500\"/>\n",
    "\n",
    "**Generation in NLP** \n",
    "\n",
    "Current trend in NLP is to use **sequence-to-sequence models**/encoder-decoder architectures. \n",
    "\n",
    "> RNNs \n",
    "\n",
    "*ELMO*\n",
    "\n",
    "<img src=\"https://richliao.github.io/images/Standard_Seq2Seq.png\" width=\"500\"/>\n",
    "\n",
    "> Transformers\n",
    "\n",
    "*BERT, GPT(1-3), XLNet, BigBird, Linformer, Performer, Reformer, Perceiver, ...*\n",
    "\n",
    "<img src=\"https://richliao.github.io/images/Standard_Seq2Seq.png\" width=\"500\"/>\n",
    "\n",
    "**The BIG tasks in NLP**\n",
    "\n",
    "> Natural Language Understanding: Semantic Parsing, Knowledge Graphs, Knowledge Representation, Grounding, Multi-Modal Grounding\n",
    "\n",
    "> Natural Language Generation: Text Generation, Dialogue, Multi-Modal-Dialogue, \n",
    "\n",
    "<img src=\"https://image.ibb.co/kkaABL/NLP-768x356.png\" width=\"500\"/>\n",
    "\n",
    "\n",
    "```\n",
    "QUESTION TO YOU: Is there anybody working with transformer architectures already, if yes, which one?  \n",
    " \n",
    "A: yes, GPT-3\n",
    "\n",
    "B: no\n",
    "```\n",
    "\n",
    "#### LEARNING in NLP \n",
    "\n",
    "Machine Learning in NLP is trending from shallow to deep learning in lots of tasks (NLU, NLG). \n",
    "\n",
    "<img src=\"https://i.pinimg.com/originals/af/57/17/af5717af56be87f930991111ed36209b.png\" width=\"500\"/>\n",
    "\n",
    "```\n",
    "QUESTION TO YOU: Philosphical Question: Computer Vision people use raw images to detect features, do you think we can reach that with raw texts as well. Put a pro or a contra argument ?  \n",
    "\n",
    "A: pro: single letter based embeddings\n",
    "\n",
    "B: contra: models need to be able to aquire the skill of READING\n",
    "```\n",
    "\n",
    "##### SHALLOW LEARNING ALGORITHMS in NLP \n",
    "\n",
    "Shallow learning in NLP is getting more and more replaced by deep learning approaches. Concerning sentiment classification or spam detection people also \n",
    "combine deep methods for feature extraction(e.g. NNs) with shallow methods(e.g. SVM) for classification. \n",
    "\n",
    "<img src=\"https://www.kdnuggets.com/wp-content/uploads/cnn-architecture.jpg\" width=\"500\"/>\n",
    "\n",
    "**Parsing** is still done using statistical methods. \n",
    "\n",
    "<img src=\"https://devopedia.org/images/article/243/7605.1575613267.png\" width=\"500\"/>\n",
    "\n",
    "```\n",
    "QUESTION TO YOU: Are there people from a theoretical linguistic background? What parsing methods do you use?  \n",
    "\n",
    "A:\n",
    "\n",
    "B:\n",
    "```\n",
    "\n",
    "\n",
    "##### DEEP LEARNING ALGORITHMS in NLP \n",
    "\n",
    "Current Trend: *TRANSFORMERS*\n",
    "\n",
    "<img src=\"http://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png\" width=\"500\"/>\n",
    "\n",
    "```\n",
    "QUESTION TO YOU: Is there people having experience with transformers already? E.g. using the Huggingface-Library?\n",
    "\n",
    "A: yes, SBERT\n",
    "\n",
    "B: no\n",
    "```\n",
    "\n",
    "\n",
    "##### TRAINING in NLP \n",
    "\n",
    "Training in NLP does not differ that much from training in ML in general. Get your data. Define your task. Define/Select your loss/reward metric. Run an optimizer on it (which almost\n",
    "always is gradient descent in combination with error backpropagation). \n",
    "\n",
    "```\n",
    "QUESTION TO YOU:  \n",
    "\n",
    "A:\n",
    "\n",
    "B:\n",
    "```\n",
    "\n",
    "##### EVALUATION in NLP\n",
    "\n",
    "* Most common metrics in text generation: BLEU, ROUGE, SPICE, ...\n",
    "\n",
    "<img src=\"data:image/png;base64,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\" width=\"500\"/>\n",
    "\n",
    "* Task Specific Metrics required often times\n",
    "\n",
    "<img src=\"https://res.cloudinary.com/practicaldev/image/fetch/s--aXg_8xot--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/i/lgvtxn3y7pkaxke1k7va.png\" width=\"500\"/>\n",
    "\n",
    "* Evaluation is still a topic of high discussion (e.g. evaluating dialogue agents, human in the loop evaluation, ...)\n",
    "\n",
    "```\n",
    "QUESTION TO YOU: If you have an idea on how to evaluate dialogue systems better, shout out, there's a paper for you!  \n",
    "\n",
    "A: yes, my idea: constructing ontologies/knowledge graphs and compute meaning overlap\n",
    "\n",
    "B: \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1967be6",
   "metadata": {},
   "source": [
    "## C) Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6eab77",
   "metadata": {},
   "source": [
    "#### MACHINE LEARNING IN NLP\n",
    "\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c15337d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# check GPU reachability \n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e3c39ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 25000\n",
      "Number of testing examples: 25000\n",
      "{'text': ['Delightful', 'Disney', 'film', 'with', 'Angela', 'Lansbury', 'in', 'fine', 'form', 'as', 'a', 'middle', 'age', 'spinster', 'whose', 'interest', 'turns', 'to', 'witchcraft', 'in', 'World', 'War', '11', 'England.<br', '/><br', '/>Lansbury', 'was', 'about', 'age', '51', 'at', 'the', 'time', 'of', 'the', 'film', 'and', 'she', 'is', 'just', 'ideal', 'for', 'the', 'part.', 'She', 'is', 'Jessica', 'Fletcher', 'again', 'but', 'this', 'time', \"it's\", 'for', 'the', 'benefit', 'or', 'children', 'and', 'for', 'mother', 'England', 'during', 'a', 'time', 'of', 'great', 'peril.<br', '/><br', '/>The', 'film', 'follows', 'the', 'adventures', 'of', 'Miss', 'Price', '(Lansbury)', 'and', 'David', 'Tomlinson', 'as', 'the', 'professor', 'of', 'witchcraft', 'in', 'trying', 'to', 'obtain', 'certain', 'information', 'on', 'sorcery.', 'Those', '3', 'little', 'darlings', 'sent', 'to', 'live', 'with', 'Price', 'to', 'escape', 'the', 'London', 'bombings', 'are', 'just', 'wonderful', 'in', 'this', 'enchanting', 'film', 'for', 'all', 'of', 'us', 'regardless', 'of', 'age.<br', '/><br', '/>Too', 'bad', 'that', 'Tessie', \"O'Shea,\", 'Roddy', 'McDowall', 'and', 'Sam', 'Jaffe', 'are', 'given', 'so', 'little', 'to', 'do', 'in', 'this', 'endearing', 'film.<br', '/><br', '/>I', 'really', 'thought', 'of', 'the', 'Ben', 'Stiller', 'film-\"Night', 'at', 'the', 'Museum,\"', 'at', 'the', 'end', 'of', 'the', 'film', 'when', 'the', 'relics', 'come', 'to', 'life', 'to', 'do', 'battle', 'with', 'the', 'Nazi', 'invasion', 'in', 'the', 'small', 'British', 'coastal', 'town.'], 'label': 'pos'}\n"
     ]
    }
   ],
   "source": [
    "# LOAD DATASET\n",
    "import torch\n",
    "from torchtext import data\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "TEXT = data.Field()\n",
    "LABEL = data.LabelField(dtype = torch.float)\n",
    "\n",
    "# get the imdb dataset\n",
    "from torchtext import datasets\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')\n",
    "\n",
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0779efbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL SELECTION\n",
    "\n",
    "# we use an RNN as our model for sentiment classification\n",
    "import torch.nn as nn\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim) # Pre-trained Embedding ? \n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim) # lSTM, Bi-directional LSTM?, Transformer ? \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        #text = [sent len, batch size]\n",
    "        embedded = self.embedding(text)\n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        #output = [sent len, batch size, hid dim]\n",
    "        #hidden = [1, batch size, hid dim]\n",
    "        assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n",
    "        \n",
    "        return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2db906f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 12250\n",
      "Number of validation examples: 5250\n",
      "Number of testing examples: 25000\n",
      "Unique tokens in TEXT vocabulary: 25002\n",
      "Unique tokens in LABEL vocabulary: 2\n",
      "[('the', 140768), ('a', 75919), ('and', 74669), ('of', 69617), ('to', 64875), ('is', 50547), ('in', 42021), ('I', 32371), ('that', 31493), ('this', 27890), ('it', 26519), ('/><br', 25026), ('was', 22602), ('as', 20800), ('with', 20393), ('for', 20134), ('but', 16463), ('The', 16434), ('on', 15073), ('movie', 14955)]\n",
      "['<unk>', '<pad>', 'the', 'a', 'and', 'of', 'to', 'is', 'in', 'I']\n",
      "defaultdict(None, {'neg': 0, 'pos': 1})\n"
     ]
    }
   ],
   "source": [
    "# TRAINING AN EMBEDDING MODEL \n",
    "# using pre trained word embeddings https://medium.com/@martinpella/how-to-use-pre-trained-word-embeddings-in-pytorch-71ca59249f76\n",
    "\n",
    "# TRAINING A CLASSIFIER\n",
    "\n",
    "# split our data into train and validation data\n",
    "import random\n",
    "train_data, valid_data = train_data.split(random_state = random.seed(SEED))\n",
    "\n",
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')\n",
    "\n",
    "# build our vocabulary\n",
    "MAX_VOCAB_SIZE = 25_000\n",
    "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")\n",
    "\n",
    "# print some statistics about the data set\n",
    "print(TEXT.vocab.freqs.most_common(20))\n",
    "print(TEXT.vocab.itos[:10])\n",
    "print(LABEL.vocab.stoi)\n",
    "\n",
    "# get data iterators for train, valid and test\n",
    "BATCH_SIZE = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6bd7ff49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            loss = criterion(predictions, batch.label)\n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4130ab9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 2,592,105 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# model parameters\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "# instantiate our model \n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "# choose an optimizer\n",
    "import torch.optim as optim\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
    "# choose a loss function / criterion\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# move model and loss function go gpus to increase training speed\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "040aee76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.44%\n",
      "\t Val. Loss: 0.697 |  Val. Acc: 49.40%\n",
      "Epoch: 02 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.57%\n",
      "\t Val. Loss: 0.697 |  Val. Acc: 49.25%\n",
      "Epoch: 03 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.60%\n",
      "\t Val. Loss: 0.696 |  Val. Acc: 49.30%\n",
      "Epoch: 04 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.58%\n",
      "\t Val. Loss: 0.696 |  Val. Acc: 49.34%\n",
      "Epoch: 05 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.47%\n",
      "\t Val. Loss: 0.697 |  Val. Acc: 49.30%\n"
     ]
    }
   ],
   "source": [
    "# run training\n",
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        #torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4740c1ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3cafc9b64000>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#model.load_state_dict(torch.load('tut1-model.pt'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluate' is not defined"
     ]
    }
   ],
   "source": [
    "# EVALUATING A CLASSIFIER \n",
    "\n",
    "#model.load_state_dict(torch.load('tut1-model.pt'))\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38aa665a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LITTLE HOMEWORK: \n",
    "\n",
    "# For the little homework you can choose between 2 tasks: \n",
    "# a) Train and evaluate 3 scikit learn classifiers on a data set from openml.org\n",
    "# OR\n",
    "# b) Use my RNN trained on the IMDB data set used for sentiment classification (see above) and \n",
    "# push the score (current=0.5, your goal should be at least 0.8 ACCURACY)\n",
    "\n",
    "# AS A GUIDE YOU CAN FOLLOW THE STEPS BELOW:\n",
    "\n",
    "#-----\n",
    "\n",
    "## 1. DATA\n",
    "# Find a data set that you like on: https://openml.org/ (if possible choose binary classification datasets because you can visualize them nicely)\n",
    "# OR\n",
    "# Use my IMDB data set from above\n",
    "\n",
    "## How does your data look like? \n",
    "## How does a single data row look like? \n",
    "## How does your target vector look like? \n",
    "\n",
    "## 2. MODEL \n",
    "## Choose 3(!) different models from: https://scikit-learn.org/stable/supervised_learning.html \n",
    "# OR\n",
    "# improve my RNN (e.g. using LSTM, BidirectionalLSTM, ....)\n",
    "\n",
    "## 3. TRAINING\n",
    "# train your models on your training data\n",
    "# plot the classification surfaces or the learning curves (openml task) \n",
    "# OR\n",
    "# plot only the learning curves (IMDB task)\n",
    "\n",
    "## 4. EVALUATION\n",
    "## Compute the LOSS\n",
    "## Compute the ACCURACY\n",
    "## Apply CROSS VALIDATION\n",
    "## Compute a CONFUSION MATRIX\n",
    "## Compute and visualize ROC AUC\n",
    "\n",
    "## 5. SEND your data to henrik.voigt@uni-jena.de by using the naming convention: HOMEWORK_01_FIRSTNAME_LASTNAME.ipynb until May 19th 00.00 AM/midnight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0285203",
   "metadata": {},
   "source": [
    "# TODO's\n",
    "\n",
    "1. Choose a date for your paper discussion/presentation by **Friday, 07.05.2021 12.00 PM/noon** at https://terminplaner4.dfn.de/XP1MLFhVyz1eA44a\n",
    "\n",
    "2. Send your finished presentations (+ possibly annotated paper) by **Monday 12.00 AM/midnight** via email to henrik.voigt@uni-jena.de\n",
    "\n",
    "3. SEND your little HOMEWORK to henrik.voigt@uni-jena.de by using the naming convention: HOMEWORK_01_FIRSTNAME_LASTNAME.ipynb until May 19th 00.00 AM/midnight\n",
    "\n",
    "***\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env-kernel",
   "language": "python",
   "name": "pytorch-env-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
