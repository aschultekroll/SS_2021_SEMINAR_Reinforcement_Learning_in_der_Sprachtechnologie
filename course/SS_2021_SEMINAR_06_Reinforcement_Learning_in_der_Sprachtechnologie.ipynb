{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab0bb0dd",
   "metadata": {},
   "source": [
    "# SS 2021 SEMINAR 06 Reinforcement Learning in der Sprachtechnologie\n",
    "## Practice Session: RL Examples in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cfde65",
   "metadata": {},
   "source": [
    "### Announcements\n",
    "\n",
    "#### Homework\n",
    "        \n",
    "* thanks for the received notebooks!\n",
    "\n",
    "* closer look on this next week\n",
    "\n",
    "* clarification: important thing is to understand the general ML workflow, NOT to build the most difficult MODEL (different background of students)\n",
    "\n",
    "#### Paper Presentations\n",
    "\n",
    "* clarification: idea is to CHALLENGE yourself to understand a scientific paper! the requirement is explicitly NOT to totally understand it in detail, but to see how far you can get. You're welcome to pose open questions to the community in the seminar!\n",
    "\n",
    "* use 2 minute papers for inspiration\n",
    "\n",
    "* find a TOPIC that inspires you/catches you!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e3ee29",
   "metadata": {},
   "source": [
    "### A) Paper Presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a307d73b",
   "metadata": {},
   "source": [
    "### Title: Depression Detection on Social Media with Reinforcement Learning\n",
    "\n",
    "#### Link:\n",
    "[Depression Detection on Social Media with Reinforcement Learning](https://link.springer.com/chapter/10.1007%2F978-3-030-32381-3_49)\n",
    "\n",
    "#### Summary:\n",
    "\n",
    "This paper explores the potential of using only the textual information to detect depression based on the content users posted on social media sites.\n",
    "Since users may post a variety of different kinds of content, only a small number of posts are relevant to the signs and symptoms of depression.\n",
    "They propose the use of reinforcement learning method to automatically select the indicator posts from the historical posts of\n",
    "users. After the explanation of the developed model, the authors evaluate it compared to other state-of-the-art methods.\n",
    "\n",
    "#### Problem/Task/Question:\n",
    "Can a reinforcement learning method enhance the use of textual information for depression detection?\n",
    "\n",
    "For this task the following partial problems need to be addressed:\n",
    "- small number of relevant posts\n",
    "    - because of that relevant posts will be selected, before classifying based on the selection\n",
    "- existing database annotations are only available on a user level\n",
    "    - the parameter update is performed after a user classification\n",
    "\n",
    "#### Solution/Idea/Model/Approach:\n",
    "![alt text](https://media.springernature.com/original/springer-static/image/chp%3A10.1007%2F978-3-030-32381-3_49/MediaObjects/489562_1_En_49_Fig2_HTML.png \"\")\n",
    "\n",
    "**Overall Structure:**\n",
    "For each Tweet in the historical sequence:\n",
    "- compute Tweet representations with word embeddings and a one layer LSTM\n",
    "- the Agent decides on an Action to select the current post or not ( {1,0} )\n",
    "    based on the current state (current post, selected and irrelevant post)\n",
    "- this decision is attained by sampling from the multinomial distribution\n",
    "\n",
    "after selections in one episode are made:\n",
    "- The average of the indicator post set is calculated.\n",
    "- This representation is further processed by a multilayer perceptron with dropout.\n",
    "- After that a sigmoid non-linear layer converts the output into a probability distribution.\n",
    "- based on that the classifier and agent can be trained\n",
    "\n",
    "\n",
    "training:\n",
    "- The agent is trained using standard reinforcement learning algorithm called REINFORCE.\n",
    "- The objective  of  training  the  agent  is  maximizing  the expected reward under the distribution of the selection policy.\n",
    "- With the REINFORCE algorithm the gradient can be approximated for the agent and the classifier can be treated as a straightforward classification problem.\n",
    "- So that on both modules backpropagation can be applied.\n",
    "\n",
    "dataset:\n",
    "- 1,402 depressed users and 5,160 non-depressed users with 4,245,727 tweets\n",
    "- users were labeled as depressed if their anchor tweets satisfied the strict pattern “(I’m/ I was/ I am/ I’ve been) diagnosed with depression”\n",
    "- non-depressed users were labeled if they had never posted any tweet containing the character string “depress”\n",
    "\n",
    "**reinforcement learning spaces:**\n",
    "\n",
    "**state:**\n",
    "- current post\n",
    "- average pooling of selected posts\n",
    "- average pooling of irrelevant posts\n",
    "- state space: post x avg(selected posts) x avg(irrelevant posts)\n",
    "\n",
    "**action:**\n",
    "- decision weather a post is selected(relevant)\n",
    "- action space: {1,0}\n",
    "- policy: select the most likely relevant posts, so that the reward from the classifier increases\n",
    "\n",
    "**reward**\n",
    "- the likelihood of the ground truth after finishing all the selections of the i-th user\n",
    "- to encourage the model to delete more posts, additionally a regularization to limit the number of selected posts is included\n",
    "\n",
    "**transition**\n",
    "- adding load the next post and add the last to the selected or irrelevant group (and calculate the averages/state space)\n",
    "\n",
    "\n",
    "#### Results:\n",
    "- CNN/LSTM + RL methods achieve the best performance,with a value of more than 87% for the F1-measure\n",
    "\n",
    "**Utility of selected posts:**\n",
    "- the authors compared the baseline models with three settings: original dataset, selected dataset and unchosen dataset\n",
    "- on the selected dataset the models can achieve almost 2.4% better scores and the error reduction rate was more than 9%\n",
    "\n",
    "![alt text](https://media.springernature.com/original/springer-static/image/chp%3A10.1007%2F978-3-030-32381-3_49/MediaObjects/489562_1_En_49_Fig3_HTML.png)\n",
    "\n",
    "**Robustness Analysis in Realistic Scenarios:**\n",
    "\n",
    "- the authors additionally performed analysis for imbalanced data sets and additional noisy posts\n",
    "\n",
    "imbalanced data sets:\n",
    "\n",
    "![alt text](https://media.springernature.com/original/springer-static/image/chp%3A10.1007%2F978-3-030-32381-3_49/MediaObjects/489562_1_En_49_Fig4_HTML.png)\n",
    "\n",
    "the method achieved a stable and outstanding performance even though there is only a very low proportion of users with depression\n",
    "\n",
    "additional noisy posts:\n",
    "\n",
    "![alt text](https://media.springernature.com/original/springer-static/image/chp%3A10.1007%2F978-3-030-32381-3_49/MediaObjects/489562_1_En_49_Fig5_HTML.png)\n",
    "\n",
    "- the performance of  the  models  decreased  to  various degrees\n",
    "-  however, as the number of posts increased, so did the advantage of the proposed model\n",
    "- at the 90 point, our model outperforms attention-based model over 13% in F1 score\n",
    "\n",
    "- the noise are posts from an unlabelled depression-candidate dataset\n",
    "- users are included in this dataset if their anchor tweets loosely contained the character string “depress”\n",
    "\n",
    "#### Critical Discussion:\n",
    "\n",
    "* **+** clear structure and visualisation made understanding relatively easy\n",
    "* **+** the proposed method shows the integration of RL methods in larger natural language processing systems\n",
    "\n",
    "* the paper addresses people with a background in NLP.\n",
    "To understand the used functions in detail, the reader must be pretty familiar with topics like: word embeddings, neural networks and reinforcement learning\n",
    "\n",
    "\n",
    "* **-** the labels of the used dataset are constructed via a strict pattern, which might not depict the ground truth\n",
    "* **-** the noisy would contain more depressed users than random sampling would, by that the distribution of the noise data is not know.\n",
    "Because of that the evaluation of the noise data could contain an unnatural bias\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfeec88",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc7ca6b",
   "metadata": {},
   "source": [
    "## B) Practice Session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34194c0e",
   "metadata": {},
   "source": [
    "### RL Examples \n",
    "\n",
    "Before starting with concrete examples, let us do a short resume of the important RL concepts that we need to consider and then see, how they apply in examples. \n",
    "\n",
    "\n",
    "**Concepts we considered:** \n",
    "\n",
    "Environment\n",
    "\n",
    "State Space\n",
    "\n",
    "Reward (function)\n",
    "\n",
    "Return\n",
    "\n",
    "Markov Processes\n",
    "\n",
    "Markov Decision Processes\n",
    "\n",
    "Transition Function\n",
    "\n",
    "Reward (function)\n",
    "\n",
    "Value Functions\n",
    "\n",
    "State-Value-Function\n",
    "\n",
    "Action-Value Function\n",
    "\n",
    "Agent\n",
    "\n",
    "Action Space\n",
    "\n",
    "Observation Space, \n",
    "\n",
    "Policy\n",
    "\n",
    "***\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Question: Why do we need **Markov Decison Processes**?\n",
    "\n",
    "Markov Process(MP): (Transition Function, State Space, Init State)\n",
    "\n",
    "# [A|B|C]\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/437/1*SUUir-VGHy2OFqbpKxwuJA.png\" width=\"500\"/>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Markov Decison Process(MDP): = (Transition Function, State Space, Init State, Reward Function Action Space)\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/a/ad/Markov_Decision_Process.svg/400px-Markov_Decision_Process.svg.png\" width=\"500\"/>\n",
    "\n",
    "<img src=\"https://i1.wp.com/neptune.ai/wp-content/uploads/Q-table.png?resize=1024%2C448&ssl=1\" width=\"500\"/>\n",
    "\n",
    "* If someone gives me this tuple, Richard BELLMANN proved, that I can solve any MDP **OPTIMALLY** using **Policy Iteration or Value Iteration** by iteratively improving **VALUE FUNCTIONS** (state-value function, state-action-value function)\n",
    "\n",
    "* This gives us the fundamental to be sure, building a **STATE-VALUE FUNCTION** based on RETURN is actually a valid approach for finding an optimal POLICY\n",
    "\n",
    "<img src=\"http://oneraynyday.github.io/assets/mdparrow.png\" width=\"500\"/>\n",
    "\n",
    "* For building a STATE-VALUE-FUNCTION we need to build a **STATE-ACTION-VALUE FUNCTION** first. \n",
    "\n",
    "<img src=\"https://miro.medium.com/max/384/1*RhO9Ulh5nF_zc2pcgBHzAg.png\" width=\"500\"/>\n",
    "\n",
    "\n",
    "In the following we are going to take a detailed look on concrete examples in levels of increasing granularity, starting by simple text based scenarios, going over table based ones heading to concrete implementations in python code.\n",
    "\n",
    "#### Examples I \n",
    "\n",
    "\n",
    "***\n",
    ">Problem: You’re training your dog to sit. \n",
    "\n",
    "> Agent: The part of your brain that makes decisions. \n",
    "\n",
    "> Environment: Your dog, the treats, your dog’s paws, the loud neighbor,\n",
    "and so on. \n",
    "\n",
    ">Actions: Talk to your dog. Wait for dog’s reaction. Move your hand. Show\n",
    "treat. Give treat. Pet. \n",
    "\n",
    ">Observations: Your dog is paying attention to you. Your dog is\n",
    "getting tired. Your dog is going away. Your dog sat on command.\n",
    "***\n",
    "\n",
    "***\n",
    ">Problem: Your dog wants the treats you have. \n",
    "\n",
    ">Agent: The part of your dog’s brain\n",
    "that makes decisions. \n",
    "\n",
    ">Environment: You, the treats, your dog’s paws, the loud neighbor, and so on. \n",
    "\n",
    ">Actions: Stare at owner. Bark. Jump at owner. Try to steal the\n",
    "treat. Run. Sit. \n",
    "\n",
    ">Observations: Owner keeps talking loud at dog. Owner is showing\n",
    "the treat. Owner is hiding the treat. Owner gave the dog the treat.\n",
    "***\n",
    "\n",
    "***\n",
    ">Problem: A trading agent investing in the stock market. \n",
    "\n",
    ">Agent: The executing DRL\n",
    "code in memory and in the CPU. \n",
    "\n",
    ">Environment: Your internet connection, the\n",
    "machine the code is running on, the stock prices, the geopolitical uncertainty, other\n",
    "investors, day traders, and so on. \n",
    "\n",
    ">Actions: Sell n stocks of y company. Buy n stocks of y company. Hold. \n",
    "\n",
    ">Observations: Market is going up. Market is going down. There\n",
    "are economic tensions between two powerful nations. There’s danger of war in the continent. A global pandemic is wreaking havoc in the entire world.\n",
    "***\n",
    "\n",
    "***\n",
    "> Problem: You’re driving your car. \n",
    "\n",
    ">Agent: The part of your brain that makes\n",
    "decisions. \n",
    "\n",
    ">Environment: The make and model of your car, other cars, other drivers,\n",
    "the weather, the roads, the tires, and so on. \n",
    "\n",
    ">Actions: Steer by x, accelerate by y. Break\n",
    "by z. Turn the headlights on. Defog windows. Play music. \n",
    "\n",
    ">Observations: You’re approaching your destination. There’s a traffic jam on Main Street. The car next to you is driving.\n",
    "***\n",
    "***\n",
    "#### Examples II\n",
    "\n",
    "***\n",
    "Game: Hot or Cold\n",
    "\n",
    "<img src=\"https://assets.codepen.io/237169/internal/screenshots/pens/OVpbBa.default.png?fit=cover&format=auto&ha=false&height=540&quality=75&v=2&version=1435097797&width=960\" width=\"500\"/>\n",
    "\n",
    "Problem: Guess a randomly selected number using hints.\n",
    "\n",
    "Observation Space: Int range 0–3. 0 means no guess yet\n",
    "submitted, 1 means guess is lower than the\n",
    "target, 2 means guess is equal to the target,\n",
    "and 3 means guess is higher than the target.\n",
    "\n",
    "Sample Observation: 2\n",
    "\n",
    "Action Space: Float from –2000.0–2000.0. The float number the agent is guessing.\n",
    "\n",
    "Sample Action: –909.37\n",
    "\n",
    "Reward Function: The reward is the negative log of the distance the agent has guessed toward the target.\n",
    "***\n",
    "\n",
    "***\n",
    "Game: Cart Pole\n",
    "\n",
    "<img src=\"https://gym.openai.com/videos/2019-10-21--mqt8Qj1mwo/CartPole-v1/poster.jpg\" width=\"500\"/>\n",
    "\n",
    "Problem: Balance a\n",
    "pole in a\n",
    "cart.\n",
    "\n",
    "Observation Space: A four-element vector\n",
    "with ranges: from [–4.8,\n",
    "–Inf, –4.2, –Inf ] to [4.8,\n",
    "Inf, 4.2, Inf ].\n",
    "First element is the cart\n",
    "position, second is the\n",
    "cart velocity, third is\n",
    "pole angle in radians,\n",
    "fourth is the pole velocity at the tip\n",
    "\n",
    "Sample Observation: [–0.16,–1.61, 0.17,2.44]\n",
    "\n",
    "Action Space: Int range 0–1.\n",
    "0 means push\n",
    "cart left, 1\n",
    "means push\n",
    "cart right.\n",
    "\n",
    "Sample Action: 0\n",
    "\n",
    "Reward Function: The reward is\n",
    "1 for every\n",
    "step taken,\n",
    "including\n",
    "the termina-\n",
    "tion step.\n",
    "***\n",
    "\n",
    "***\n",
    "Game: Lunar Lander\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1346/1*i7lxpgt2K3Q8lgEPJu3_xA.png\" width=\"500\"/>\n",
    "\n",
    "Problem: Navigate a\n",
    "lander to its\n",
    "landing\n",
    "pad.\n",
    "\n",
    "Observation Space: An eight-element vec-\n",
    "tor with ranges: from\n",
    "[–Inf, –Inf, –Inf, –Inf,\n",
    "–Inf, –Inf, 0, 0] to [Inf,\n",
    "Inf, Inf, Inf, Inf, Inf, 1, 1].\n",
    "First element is the x\n",
    "position, the second\n",
    "the y position, the third\n",
    "is the x velocity, the\n",
    "fourth is the y velocity,\n",
    "fifth is the vehicle’s\n",
    "angle, sixth is the\n",
    "angular velocity, and\n",
    "the last two values are\n",
    "Booleans indicating\n",
    "legs contact with the\n",
    "ground.\n",
    "\n",
    "Sample Observation: [0.36 , 0.23,\n",
    "–0.63, –0.10,\n",
    "–0.97, –1.73,\n",
    "1.0, 0.0]\n",
    "\n",
    "Action Space: Int range 0–3.\n",
    "No-op (do\n",
    "nothing), fire\n",
    "left engine,\n",
    "fire main\n",
    "engine, fire\n",
    "right engine.\n",
    "\n",
    "Sample Action: 2\n",
    "\n",
    "Reward Function: Reward for\n",
    "landing is\n",
    "200. There’s a\n",
    "reward for\n",
    "moving from\n",
    "the top to\n",
    "the landing\n",
    "pad, for\n",
    "crashing or\n",
    "coming to\n",
    "rest, for each\n",
    "leg touching\n",
    "the ground,\n",
    "and for firing\n",
    "the engines..\n",
    "***\n",
    "\n",
    "***\n",
    "Game: Pong\n",
    "\n",
    "<img src=\"https://www.signalpop.com/wp-content/uploads/2018/10/pong-1.png\" width=\"500\"/>\n",
    "\n",
    "Problem: Bounce the\n",
    "ball past\n",
    "the oppo-\n",
    "nent, and\n",
    "avoid let-\n",
    "ting the\n",
    "ball pass\n",
    "you.\n",
    "\n",
    "Observation Space: A tensor of shape 210,\n",
    "160, 3.\n",
    "Values ranging 0–255.\n",
    "Represents a game\n",
    "screen image.\n",
    "\n",
    "Sample Observation: [[[246, 217,\n",
    "64], [ 55,\n",
    "184, 230],\n",
    "[ 46, 231,\n",
    "179], . . .,\n",
    "[ 28, 104,\n",
    "249], [ 25, 5,\n",
    "22], [173,\n",
    "186, 1]],\n",
    ". . .]]\n",
    "\n",
    "Action Space: Int range 0–5.\n",
    "Action 0 is\n",
    "No-op, 1 is\n",
    "Fire, 2 is up, 3\n",
    "is right, 4 is\n",
    "Left, 5 is\n",
    "Down.\n",
    "Notice how\n",
    "some actions\n",
    "don’t affect\n",
    "the game in\n",
    "any way. In\n",
    "reality the\n",
    "paddle can\n",
    "only move up\n",
    "or down, or\n",
    "not move.\n",
    "\n",
    "Sample Action: 3\n",
    "\n",
    "Reward Function: The reward is\n",
    "a 1 when the\n",
    "ball goes\n",
    "beyond the\n",
    "opponent,\n",
    "and a –1\n",
    "when your\n",
    "agent’s pad-\n",
    "dle misses\n",
    "the ball.\n",
    "***\n",
    "\n",
    "***\n",
    "Game: Humanoid Robot\n",
    "\n",
    "<img src=\"https://images.livemint.com/img/2021/01/22/1140x641/2020-12-30T104653Z_346921282_RC2MXK994162_RTRMADP_3_TECH-ROBOTS-BOSTON-DYNAMICS-DANCE_1611299330093_1611299392327.JPG\" width=\"500\"/>\n",
    "\n",
    "Problem: Make robot\n",
    "run as fast\n",
    "as possible\n",
    "and not fall.\n",
    "\n",
    "Observation Space: A 44-element (or more,\n",
    "depending on the\n",
    "implementation)\n",
    "vector.\n",
    "Values ranging from\n",
    "–Inf to Inf.\n",
    "Represents the positions and velocities of\n",
    "the robot’s joints.\n",
    "\n",
    "Sample Observation: [0.6, 0.08,\n",
    "0.9, 0. 0, 0.0,\n",
    "0.0, 0.0, 0.0,\n",
    "0.045, 0.0,\n",
    "0.47, . . . ,\n",
    "0.32, 0.0,\n",
    "–0.22, . . . , 0.]\n",
    "\n",
    "Action Space: A 17-element\n",
    "vector.\n",
    "Values rang-\n",
    "ing from –Inf\n",
    "to Inf.\n",
    "Represents\n",
    "the forces to\n",
    "apply to the\n",
    "robot’s joints.\n",
    "\n",
    "Sample Action: [–0.9,\n",
    "–0.06,\n",
    "0.6, 0.6,\n",
    "0.6,\n",
    "–0.06,\n",
    "–0.4,\n",
    "–0.9,\n",
    "0.5,\n",
    "–0.2,\n",
    "0.7,\n",
    "–0.9,\n",
    "0.4,\n",
    "–0.8,\n",
    "–0.1,\n",
    "0.8,\n",
    "–0.03]\n",
    "\n",
    "Reward Function: The reward is\n",
    "calculated\n",
    "based on\n",
    "forward\n",
    "motion with\n",
    "a small\n",
    "penalty to\n",
    "encourage a\n",
    "natural gait.\n",
    "***\n",
    "\n",
    "<br>\n",
    "\n",
    "**WHERE ARE THE TRANSITION FUNCTIONS?**: \n",
    "\n",
    "The transition functions are the **PHYSICS/RULES OF THE GAME**. That means to take a look ad the transition functions, we would have to take a look at the underlying code of the games. \n",
    "\n",
    "For the cart pole environments, this would be e.g. a small python file that defines the mass of the cart and the pole and implements the basic physics equations, so that the environment changes/transits at each time step according to these. \n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2784ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL in LANGUAGE TASKS\n",
    "\n",
    "# LANGUAGE GAMES being able to be modeled by RL: \n",
    "# Translation Agent: see sentence -> action space vocab -> reward based on the appropriateness of the translation. \n",
    "# Language Learning: environment with sounds, words, images -> reward from teacher?\n",
    "# Google Assistant/Personal Assistant: user asks question -> assistant gives answer -> QA-Dialogue!!!\n",
    "# Information Retrieval: explanation creation -> SCORING => adaptive explanations/generations\n",
    "# Human Language Learning: producing sounds, BIAS(no,..), GOALS (achieve common ground, understanding, does he/she understand me?), \n",
    "\n",
    "# FUTURE SESSION: language immergence, how is language immerging in an environment -> why and how do agents exchange sounds to ACHIEVE THINGS/GOALS?!!! (deepmind group)\n",
    "\n",
    "# ACTION SPACE: [VOCABULARY]\n",
    "# ACTION: [] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8731c07",
   "metadata": {},
   "source": [
    "## C) Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4995777f",
   "metadata": {},
   "source": [
    "#### Examples III\n",
    "\n",
    "Game: Frozen Lake\n",
    "\n",
    "I am using the gym environment of openai today. \n",
    "You can install it this way: \n",
    "\n",
    "`conda install -c conda-forge gym`\n",
    "\n",
    "The game we try to solve is the frozen lake game:\n",
    "<img src=\"https://camo.githubusercontent.com/f558f268f3c1a45f0a88342113476f34ce894896c30b66fdc3101c8d090a0a0a/68747470733a2f2f616e616c7974696373696e6469616d61672e636f6d2f77702d636f6e74656e742f75706c6f6164732f323031382f30332f46726f7a656e2d4c616b652e706e67\" width=\"500\"/>\n",
    "\n",
    "For this we use the openai gym environment implementation from here: \n",
    "https://gym.openai.com/envs/FrozenLake-v0/\n",
    "\n",
    "You can find the source code in this repository:\n",
    "https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py\n",
    "\n",
    "The environment is an implementation of the same interface as we implemented last week. \n",
    "\n",
    "Let's start:\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "88a6164e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Check GPU reachability \n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f78793f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym \n",
    "import random \n",
    "import time\n",
    "from abc import ABC, abstractmethod\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9df55f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an instance of the frozen lake gym environment\n",
    "env = gym.make(\"FrozenLake-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5971ee6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(16)\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "# get some information from the environment\n",
    "action_space = env.action_space\n",
    "#print(action_space)\n",
    "action_space_size = env.action_space.n\n",
    "#print(action_space_size)\n",
    "state_space = env.observation_space \n",
    "print(state_space)\n",
    "state_space_size = env.observation_space.n\n",
    "print(state_space_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "27c89e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-TABLE\n",
    "# build our action-value table | Q-TABLE\n",
    "# as you already know, the q-table looks like this\n",
    "# state | action_space\n",
    "\n",
    "q_table = np.zeros((state_space_size, action_space_size))\n",
    "#print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7a8a10da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "num_episodes = 10000\n",
    "max_steps_per_episode = 100\n",
    "num_test_episodes = 3     \n",
    "\n",
    "# q-learning | update parameters\n",
    "learning_rate = 0.1\n",
    "discount_rate = 0.99\n",
    "\n",
    "# exploration-exploitation trade off\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a2bd8e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-LEARNING\n",
    "\n",
    "# collect rewards somewhere to visualize our learning curve\n",
    "rewards_of_all_episodes = []\n",
    "\n",
    "# Training Loop\n",
    "for episode in range(num_episodes):\n",
    "    # reset/initialize the environment first\n",
    "    state = env.reset()\n",
    "    # set done back to false at the beginning of an episode\n",
    "    done = False\n",
    "    # reset our rewards collector | return for the beginning episode\n",
    "    rewards_current_episode = 0\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        # select an action\n",
    "        # use our exploration exploitation trade off -> do we explore or exploit in this timestep ?\n",
    "        exploration_rate_threshold = random.uniform(0,1)\n",
    "        if(exploration_rate_threshold > exploration_rate):\n",
    "            action = np.argmax(q_table[state, : ])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Update Q-Table Q(s,a) using the bellman update  \n",
    "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + learning_rate * (reward + discount_rate * np.max(q_table[new_state, : ]))\n",
    "        \n",
    "        # update the state to the new state\n",
    "        state = new_state\n",
    "        # collect the reward\n",
    "        rewards_current_episode += reward\n",
    "        \n",
    "        if (done == True):\n",
    "            break\n",
    "    \n",
    "    # after we finish an episode, make sure to update the exploration rate\n",
    "    # decay the exploration rate the longer the time goes on\n",
    "    exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "    # append our rewards for this episode for learning curve\n",
    "    rewards_of_all_episodes.append(rewards_current_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "937fbdde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****INFO: average reward per thousand episodes: ***** \n",
      "\n",
      "1000 :  0.05200000000000004\n",
      "2000 :  0.22600000000000017\n",
      "3000 :  0.3890000000000003\n",
      "4000 :  0.5600000000000004\n",
      "5000 :  0.6200000000000004\n",
      "6000 :  0.6820000000000005\n",
      "7000 :  0.6960000000000005\n",
      "8000 :  0.7160000000000005\n",
      "9000 :  0.7000000000000005\n",
      "10000 :  0.7000000000000005\n",
      "\n",
      "\n",
      " ***** Q-TABLE ***** \n",
      "\n",
      "[[0.60250763 0.54077859 0.53225512 0.5417312 ]\n",
      " [0.43348457 0.38128151 0.32437287 0.52260515]\n",
      " [0.44849661 0.40866906 0.36844345 0.4796271 ]\n",
      " [0.21224305 0.34816654 0.23593559 0.46583193]\n",
      " [0.61740758 0.42681705 0.35133093 0.39854066]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.1831593  0.19293863 0.37993096 0.07587284]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.44894363 0.50043263 0.38555528 0.65065883]\n",
      " [0.59576449 0.71645349 0.346324   0.34906457]\n",
      " [0.66089752 0.33333752 0.4307606  0.32201233]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.46242914 0.59221847 0.79649711 0.57853265]\n",
      " [0.70344546 0.90138206 0.80725199 0.76817448]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# LEARNING STATISTICS \n",
    "# for each episode print the stats of the episode\n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_of_all_episodes),num_episodes/1000)\n",
    "count = 1000\n",
    "print('*****INFO: average reward per thousand episodes: ***** \\n')\n",
    "for reward in rewards_per_thousand_episodes:\n",
    "    print(count, \": \", str(sum(reward/1000)))\n",
    "    count += 1000\n",
    "    \n",
    "# print our learned q-table\n",
    "print(\"\\n\\n ***** Q-TABLE ***** \\n\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f5322639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "INFO: ***** agent reached the goal. *****\n"
     ]
    }
   ],
   "source": [
    "# EVALUATION | TESTING | watching our agent play\n",
    "for episode in range(num_test_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print(\"INFO:*****EPISODE \", episode+1, \"\\n\\n\\n\")\n",
    "    time.sleep(1)\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.3)\n",
    "        \n",
    "        action = np.argmax(q_table[state, :])\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if reward == 1:\n",
    "                print(\"INFO: ***** agent reached the goal. *****\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"INFO: ***** agent died.\")\n",
    "                time.sleep(3)\n",
    "            clear_output(wait=True)\n",
    "            break\n",
    "        \n",
    "        state = new_state\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8db843",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "***\n",
    "\n",
    "Game: Taxi \n",
    "\n",
    "Find the OpenAI Gym Environment here: https://gym.openai.com/envs/Taxi-v3/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304b398c",
   "metadata": {},
   "source": [
    "### ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c8565961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOMAIN = the data_object (JSON SERIALIZABLE)\n",
    "class Environment():\n",
    "    \"\"\"The main Environment class. It encapsulates an environment with\n",
    "    arbitrary behind-the-scenes dynamics. An environment can be\n",
    "    partially or fully observed.\n",
    "    The main API methods that users of this class need to know are:\n",
    "        step\n",
    "        reset\n",
    "        render\n",
    "        close\n",
    "        seed\n",
    "    And set the following attributes:\n",
    "        action_space: The Space object corresponding to valid actions\n",
    "        observation_space: The Space object corresponding to valid observations\n",
    "        reward_range: A tuple corresponding to the min and max possible rewards\n",
    "    Note: a default reward range set to [-inf,+inf] already exists. Set it if you want a narrower range.\n",
    "    The methods are accessed publicly as \"step\", \"reset\", etc...\n",
    "    \"\"\"\n",
    "    # Set this in SOME subclasses\n",
    "    metadata = {'render.modes': []}\n",
    "    reward_range = (-float('inf'), float('inf'))\n",
    "    spec = None\n",
    "\n",
    "    def __init__(self, action_space=None, observation_space=None):\n",
    "        # Set variables\n",
    "        self.action_space = action_space\n",
    "        self.observation_space = observation_space\n",
    "    \n",
    "# REPOSITORY = the functionality interface\n",
    "class EnvironmentRepository(ABC):\n",
    "    @abstractmethod\n",
    "    def step(self, action):\n",
    "        \"\"\"Run one timestep of the environment's dynamics. When end of\n",
    "        episode is reached, you are responsible for calling `reset()`\n",
    "        to reset this environment's state.\n",
    "        Accepts an action and returns a tuple (observation, reward, done, info).\n",
    "        Args:\n",
    "            action (object): an action provided by the agent\n",
    "        Returns:\n",
    "            observation (object): agent's observation of the current environment\n",
    "            reward (float) : amount of reward returned after previous action\n",
    "            done (bool): whether the episode has ended, in which case further step() calls will return undefined results\n",
    "            info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the environment to an initial state and returns an initial\n",
    "        observation.\n",
    "        Note that this function should not reset the environment's random\n",
    "        number generator(s); random variables in the environment's state should\n",
    "        be sampled independently between multiple calls to `reset()`. In other\n",
    "        words, each call of `reset()` should yield an environment suitable for\n",
    "        a new episode, independent of previous episodes.\n",
    "        Returns:\n",
    "            observation (object): the initial observation.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"Renders the environment.\n",
    "        The set of supported modes varies per environment. (And some\n",
    "        environments do not support rendering at all.) By convention,\n",
    "        if mode is:\n",
    "        - human: render to the current display or terminal and\n",
    "          return nothing. Usually for human consumption.\n",
    "        - rgb_array: Return an numpy.ndarray with shape (x, y, 3),\n",
    "          representing RGB values for an x-by-y pixel image, suitable\n",
    "          for turning into a video.\n",
    "        - ansi: Return a string (str) or StringIO.StringIO containing a\n",
    "          terminal-style text representation. The text can include newlines\n",
    "          and ANSI escape sequences (e.g. for colors).\n",
    "        Note:\n",
    "            Make sure that your class's metadata 'render.modes' key includes\n",
    "              the list of supported modes. It's recommended to call super()\n",
    "              in implementations to use the functionality of this method.\n",
    "        Args:\n",
    "            mode (str): the mode to render with\n",
    "        Example:\n",
    "        class MyEnv(Env):\n",
    "            metadata = {'render.modes': ['human', 'rgb_array']}\n",
    "            def render(self, mode='human'):\n",
    "                if mode == 'rgb_array':\n",
    "                    return np.array(...) # return RGB frame suitable for video\n",
    "                elif mode == 'human':\n",
    "                    ... # pop up a window and render\n",
    "                else:\n",
    "                    # just raise an exception\n",
    "                    super(MyEnv, self).render(mode=mode)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def close(self):\n",
    "        \"\"\"Override close in your subclass to perform any necessary cleanup.\n",
    "        Environments will automatically close() themselves when\n",
    "        garbage collected or when the program exits.\n",
    "        \"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9c4727d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPOSITORY IMPLEMENTATION = the way how you would like to implement it\n",
    "class EnvironmentRepositoryImpl(EnvironmentRepository):\n",
    "    # Initialize / Instance Attributes\n",
    "    def __init__(self, environment):\n",
    "        # Set variables\n",
    "        self.data_object = environment\n",
    "        print('Environment initialized')\n",
    "\n",
    "    def step(self, action):\n",
    "        state = self.data_object.step(action)\n",
    "        return state\n",
    "\n",
    "    def reset(self):\n",
    "        state = self.data_object.reset()\n",
    "        return state\n",
    "\n",
    "    def render(self):\n",
    "        self.data_object.render()\n",
    "\n",
    "    def close(self):\n",
    "        state = self.data_object.close()\n",
    "\n",
    "    def get_action_space(self):\n",
    "        # get action space from api of the playground or via js in browser using selenium\n",
    "        action_space = self.data_object.action_space\n",
    "        return action_space\n",
    "\n",
    "    def get_observation_space(self):\n",
    "        # get observation space of the playground from api or via js in browser using selenium\n",
    "        observation_space = self.data_object.observation_space\n",
    "        return observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ff7200",
   "metadata": {},
   "source": [
    "### AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "517b01ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOMAIN\n",
    "class Agent():\n",
    "    # class variables\n",
    "    agent_variable = \"\"\n",
    "    # class methods\n",
    "    def __init__(self, agent_variable=\"\"):\n",
    "        self.agent_variable = agent_variable\n",
    "        \n",
    "# REPOSITORY\n",
    "class AgentRepository(ABC):\n",
    "    @abstractmethod\n",
    "    def get_action(self, state): # This is the agent's POLICY, if you will\n",
    "        \"\"\" Agent gets a state as input and returns an action \n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f808a0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPOSITORY IMPLEMENTATION\n",
    "class AgentRepositoryImpl(AgentRepository):\n",
    "    # Initializer / Instance Attributes\n",
    "    def __init__(self, environment, agent):\n",
    "        # Set variables\n",
    "        self.data_object = agent\n",
    "        self.environment = environment\n",
    "        self.action_space = self.environment.action_space\n",
    "        self.observation_space = self.environment.observation_space\n",
    "        self.action_space_size = self.environment.action_space.n\n",
    "        self.observation_space_size = self.environment.observation_space.n\n",
    "        # INIT Q-TABLE\n",
    "        self.q_table = np.zeros((self.observation_space_size, self.action_space_size))\n",
    "        # INIT AGENT PARAMETERS\n",
    "        self.learning_rate = 0.7           # Learning rate\n",
    "        self.discount_rate = 0.618         # Discounting rate\n",
    "        self.exploration_rate = 1.0        # Exploration rate\n",
    "        self.max_exploration_rate = 1.0    # Exploration probability at start\n",
    "        self.min_exploration_rate = 0.01   # Minimum exploration probability \n",
    "        self.exploration_decay_rate = 0.01 # Exponential decay rate for exploration probability\n",
    "        print('Agent initialized.')\n",
    "\n",
    "    def get_action(self, state):\n",
    "        #EXPLORATION-EXPLOITATION TRADE OFF\n",
    "        exploration_rate_threshold = random.uniform(0,1)\n",
    "        if(exploration_rate_threshold > self.exploration_rate):\n",
    "            # get action from q table\n",
    "            action = np.argmax(self.q_table[state, : ])\n",
    "        else:\n",
    "            # get random action\n",
    "            action = self.get_random_action()\n",
    "        return action\n",
    "    \n",
    "    def get_random_action(self):\n",
    "        #action_set = random.sample(self.action_space, 1)\n",
    "        #action = action_set[0]\n",
    "        action = self.action_space.sample()\n",
    "        return action\n",
    "    \n",
    "    def update_q_table(self, state, action, reward, new_state):\n",
    "        self.q_table[state, action] = self.q_table[state, action] * (1 - self.learning_rate) + self.learning_rate * (reward + self.discount_rate * np.max(self.q_table[new_state, : ]))\n",
    "        \n",
    "    def update_exploration_rate(self, episode_num):\n",
    "        self.exploration_rate = self.min_exploration_rate + (self.max_exploration_rate - self.min_exploration_rate) * np.exp(-self.exploration_decay_rate*episode_num)\n",
    "    \n",
    "    def get_exploit_action(self, state):\n",
    "        action = np.argmax(self.q_table[state, : ])\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6268ba07",
   "metadata": {},
   "source": [
    "### TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bb9a1da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "num_episodes = 50000        # Total episodes\n",
    "max_steps_per_episode = 100 # Max steps per episode\n",
    "num_test_episodes = 5     # Total test episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bed4b2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(6)\n",
      "6\n",
      "Discrete(500)\n",
      "500\n",
      "(-inf, inf)\n",
      "Environment initialized\n",
      "Agent initialized.\n"
     ]
    }
   ],
   "source": [
    "# Setting up the Environment\n",
    "\n",
    "# get the environment\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "\n",
    "# get some information from the environment\n",
    "action_space = env.action_space\n",
    "print(action_space)\n",
    "action_space_size = env.action_space.n\n",
    "print(action_space_size)\n",
    "observation_space = env.observation_space \n",
    "print(observation_space)\n",
    "observation_space_size = env.observation_space.n\n",
    "print(observation_space_size)\n",
    "reward_range = env.reward_range\n",
    "print(reward_range)\n",
    "\n",
    "environment_data_object = env #Environment(action_space, observation_space)\n",
    "environment = EnvironmentRepositoryImpl(environment_data_object)\n",
    "\n",
    "# Setting up the Agent\n",
    "agent_data_object = Agent()\n",
    "agent = AgentRepositoryImpl(environment_data_object, agent_data_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7867c864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "# collect rewards somewhere to visualize our learning curve\n",
    "rewards_of_all_episodes = []\n",
    "\n",
    "# Training Loop\n",
    "for episode in range(num_episodes):\n",
    "    # reset/initialize the environment first\n",
    "    state = environment.reset()\n",
    "    # set done back to false at the beginning of an episode\n",
    "    done = False\n",
    "    # reset our rewards collector | return for the beginning episode\n",
    "    rewards_current_episode = 0\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        # select an action\n",
    "        # use our exploration exploitation trade off -> do we explore or exploit in this timestep ?\n",
    "        action = agent.get_action(state)\n",
    "            \n",
    "        new_state, reward, done, info = environment.step(action)\n",
    "        \n",
    "        # Update Q-Table Q(s,a) using the bellman update  \n",
    "        agent.update_q_table(state, action, reward, new_state)\n",
    "        \n",
    "        # update the state to the new state\n",
    "        state = new_state\n",
    "        # collect the reward\n",
    "        rewards_current_episode += reward\n",
    "        \n",
    "        if (done == True):\n",
    "            break\n",
    "    \n",
    "    # after we finish an episode, make sure to update the exploration rate\n",
    "    # decay the exploration rate the longer the time goes on\n",
    "    agent.update_exploration_rate(episode)\n",
    "    # append our rewards for this episode for learning curve\n",
    "    rewards_of_all_episodes.append(rewards_current_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d6048132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****INFO: average reward per thousand episodes: ***** \n",
      "\n",
      "1000 :  -45.3959999999998\n",
      "2000 :  7.407999999999966\n",
      "3000 :  7.5049999999999635\n",
      "4000 :  7.351999999999967\n",
      "5000 :  7.470999999999966\n",
      "6000 :  7.561999999999962\n",
      "7000 :  7.5269999999999735\n",
      "8000 :  7.457999999999962\n",
      "9000 :  7.48799999999996\n",
      "10000 :  7.464999999999965\n",
      "11000 :  7.399999999999965\n",
      "12000 :  7.312999999999967\n",
      "13000 :  7.3119999999999585\n",
      "14000 :  7.335999999999961\n",
      "15000 :  7.421999999999961\n",
      "16000 :  7.3349999999999556\n",
      "17000 :  7.301999999999971\n",
      "18000 :  7.198999999999964\n",
      "19000 :  7.298999999999972\n",
      "20000 :  7.408999999999964\n",
      "21000 :  7.5029999999999575\n",
      "22000 :  7.471999999999971\n",
      "23000 :  7.283999999999966\n",
      "24000 :  7.396999999999963\n",
      "25000 :  7.408999999999962\n",
      "26000 :  7.49899999999996\n",
      "27000 :  7.361999999999972\n",
      "28000 :  7.333999999999961\n",
      "29000 :  7.434999999999964\n",
      "30000 :  7.4679999999999644\n",
      "31000 :  7.595999999999968\n",
      "32000 :  7.188999999999968\n",
      "33000 :  7.5039999999999605\n",
      "34000 :  7.349999999999961\n",
      "35000 :  7.42599999999996\n",
      "36000 :  7.511999999999971\n",
      "37000 :  7.358999999999963\n",
      "38000 :  7.316999999999972\n",
      "39000 :  7.415999999999971\n",
      "40000 :  7.351999999999968\n",
      "41000 :  7.441999999999956\n",
      "42000 :  7.342999999999967\n",
      "43000 :  7.316999999999961\n",
      "44000 :  7.54499999999997\n",
      "45000 :  7.552999999999964\n",
      "46000 :  7.4139999999999695\n",
      "47000 :  7.504999999999956\n",
      "48000 :  7.368999999999971\n",
      "49000 :  7.3769999999999705\n",
      "50000 :  7.327999999999974\n",
      "\n",
      "\n",
      " ***** Q-TABLE ***** \n",
      "\n",
      "[[  0.           0.           0.           0.           0.\n",
      "    0.        ]\n",
      " [ -2.5042147   -2.43401533  -2.50423222  -2.43407224  -2.32039715\n",
      "  -11.43396515]\n",
      " [ -1.84008203  -1.35777551  -1.83919178  -1.35844496  -0.57891593\n",
      "  -10.35892069]\n",
      " ...\n",
      " [ -1.94631341   0.6813468   -1.95448955  -2.0084676   -7.\n",
      "   -7.        ]\n",
      " [ -2.35377987  -2.34048369  -2.29903226  -2.13656806  -9.1\n",
      "   -9.98592646]\n",
      " [ -0.7         -0.91        -0.7         11.36        -2.71168563\n",
      "    0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# LEARNING STATISTICS \n",
    "# for each episode print the stats of the episode\n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_of_all_episodes),num_episodes/1000)\n",
    "count = 1000\n",
    "print('*****INFO: average reward per thousand episodes: ***** \\n')\n",
    "for reward in rewards_per_thousand_episodes:\n",
    "    print(count, \": \", str(sum(reward/1000)))\n",
    "    count += 1000\n",
    "    \n",
    "# print our learned q-table\n",
    "print(\"\\n\\n ***** Q-TABLE ***** \\n\")\n",
    "print(agent.q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f2fb3eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n"
     ]
    }
   ],
   "source": [
    "# EVALUATION | TESTING | watching our agent play\n",
    "for episode in range(num_test_episodes):\n",
    "    state = environment.reset()\n",
    "    done = False\n",
    "    print(\"INFO:*****EPISODE \", episode+1, \"\\n\\n\\n\")\n",
    "    time.sleep(1)\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        clear_output(wait=True)\n",
    "        environment.render()\n",
    "        time.sleep(0.3)\n",
    "        \n",
    "        action = agent.get_exploit_action(state)\n",
    "        new_state, reward, done, info = environment.step(action)\n",
    "        \n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            environment.render()\n",
    "            if reward == 20:\n",
    "                print(\"INFO: ***** agent reached the goal. *****\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"INFO: ***** agent missed the goal.\")\n",
    "                time.sleep(3)\n",
    "            clear_output(wait=True)\n",
    "            break\n",
    "        \n",
    "        state = new_state\n",
    "\n",
    "environment.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "386cb640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LITTLE HOMEWORK\n",
    "# 1. get an environment of the openai gym (e.g. cart pole, lunar lander, breakout)\n",
    "# 2. print the essential information about the environment (state space, action space, ...)\n",
    "# 3. write an agent class\n",
    "# 4. train your agent on the environment using Q-Learning (play around with the hyperparameters for your environment)\n",
    "# 5. Plot your results (average reward, q-table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b05f492",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ba69ae",
   "metadata": {},
   "source": [
    "# TODO's\n",
    "\n",
    "1. Send your finished presentations (+ possibly annotated paper) by **Monday 12.00 AM/midnight** via email to henrik.voigt@uni-jena.de\n",
    "\n",
    "2. Send your little HOMEWORK to henrik.voigt@uni-jena.de by using the naming convention: HOMEWORK_02_FIRSTNAME_LASTNAME.ipynb until **June 9th 12.00 AM/midnight**\n",
    "\n",
    "***\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env-kernel",
   "language": "python",
   "name": "pytorch-env-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
