{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb7d5c92",
   "metadata": {},
   "source": [
    "# SS 2021 SEMINAR 04 Reinforcement Learning in der Sprachtechnologie\n",
    "## Reinforcement Learning I "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320123b5",
   "metadata": {},
   "source": [
    "## A) Discussion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4149ac3",
   "metadata": {},
   "source": [
    "**Image Segmentation WITHOUT pixel-wise classification approach**\n",
    "\n",
    "* Image Segmentation Paper using GENERATIVE approaches(transformers): https://www.semanticscholar.org/paper/Axial-DeepLab%3A-Stand-Alone-Axial-Attention-for-Wang-Zhu/9167d23ad51f6fae7ce11e15fa519263c4ac9e9e\n",
    "\n",
    "**TRANSFORMERS** \n",
    "\n",
    "* BERT in detail: ChrisMcCormickAI https://www.youtube.com/channel/UCoRX98PLOsaN8PtekB9kWrw\n",
    " \n",
    "* Papers implemented LINE BY LINE(!!!): Aladdin Persson https://www.youtube.com/channel/UCkzW5JSFwvKRjXABI-UTAkQ\n",
    "\n",
    "* Huggingface: https://huggingface.co/ or here: https://github.com/huggingface/transformers\n",
    "\n",
    "* Jay Alammar: https://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "\n",
    "**Terminplaner Problem** \n",
    "\n",
    "* Please Check your dates again, we will use the first accepted (green) data on this page: https://terminplaner4.dfn.de/XP1MLFhVyz1eA44a\n",
    "\n",
    "**REMEMBER: HOMEWORK SUBMISSION DEADLINE: May 19th, MAIL: henrik.voigt@uni-jena.de, NAMING: HOMEWORK_01_FIRSTNAME_LASTNAME.ipynb**\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08450d6",
   "metadata": {},
   "source": [
    "## B) Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a02d5d",
   "metadata": {},
   "source": [
    "### REINFORCEMENT LEARNING I\n",
    "\n",
    "#### Why RL?\n",
    "\n",
    "TEASER: https://www.youtube.com/watch?v=WXuK6gekU1Y&t=9s\n",
    "\n",
    "**Reinforcement Learning is ...**\n",
    "\n",
    "```\n",
    "QUESTION TO YOU: ...?  \n",
    "\n",
    "A: learning from trial and error. \n",
    "\n",
    "B: ?\n",
    "```\n",
    "\n",
    "**David Silver**: \n",
    "\n",
    "<img src=\"https://media.wired.com/photos/5fe26be462dc9f2effae991b/master/pass/Business-Alpha-Go-David_Silver.jpg\" width=\"500\"/>\n",
    "\n",
    "> So **reinforcement learning** is the study and the science and the problem of intelligence in the form of an agent that interacts with an environment. So the problem is trying to solve is represented by some environment like the world in which that agent is situated. And the goal of RL is clear that the agent gets to take actions. Those actions have some effect on the environment and the environment gives back an observations. The agent saying, you know, this is what you see or sense, and one special thing which it gives back, it's called the reward signal, how well it's doing in the environment. And the reinforcement learning problem is to simply take actions over time so as to maximize that reward signal.\n",
    "\n",
    "> So amongst all the approaches for reinforcement learning, **deep reinforcement learning** as one family of solution methods that tries to utilize powerful representations that are offered by neural networks to represent any of these different components of the solution of the agent, like whether it's the value function or the model or the policy. The idea of deep learning is to say, well, here's a powerful toolkit that's so powerful that it's universal in the sense that it can represent any function and it can learn any function.\n",
    "\n",
    "\n",
    "**Jeff Dean**: \n",
    "\n",
    "<img src=\"https://media.wired.com/photos/5ad69141e7f4053764abb7fc/125:94/w_1195,h_899,c_limit/Jeff-Dean-FINAL.jpg\" width=\"500\"/>\n",
    "\n",
    "> The idea behind reinforcement learning is you don't necessarily know the **actions** you might take, so you explore the sequence of actions you should take by taking one that you think is a good idea and then observing how the world reacts. Like in a board game where you can react to how your opponent plays.\n",
    "\n",
    "> Reinforcement learning is the idea of being able to assign credit or blame to all the actions you took along **the way** while you were getting that reward signal.\n",
    "\n",
    "\n",
    "**Richard Sutton**\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/600/1*aIslMzbp8-olmVVQHyxBbg.jpeg\" width=\"500\"/>\n",
    "\n",
    "> Reinforcement learning is learning from **rewards**, by trial and error, during normal interaction with the world. This makes it very much like natural learning processes and unlike supervised learning, in which learning only happens during a special training phase in which a supervisory or teaching signal is available that will not be available during normal use.\n",
    "\n",
    "> Deep reinforcement learning is the combination of deep learning and reinforcement learning. These two kinds of learning address largely orthogonal issues and combine nicely. In short, reinforcement learning needs methods for approximating functions from data to implement all of its components - value functions, policies, world models, state updaters - and deep learning is the latest and most successful of recently developed **function approximators**. \n",
    "\n",
    "*\n",
    "\n",
    "Sutton is also the author of a fundamental book in RL:\n",
    "\n",
    "<img src=\"https://lh3.googleusercontent.com/proxy/mwq-z6DSq8KdCui4hvOroAuy6mm5DDPSxO8KFejMsNzXpidz87hgTQ46JuvLbRE7ZLFnnkHML6XoIwJuU39ETwJmyQQb\" width=\"200\"/>\n",
    "\n",
    "The authors make the source FREELY(!!!) available at: http://incompleteideas.net/book/the-book.html\n",
    "\n",
    "*\n",
    "\n",
    "My personal definition: \n",
    "\n",
    "**Reinforcement Learning is creating a POLICY function that enables an agent to reach a certain GOAL in an optimal way, by using sparse positiv/negative feedback.**\n",
    "\n",
    "**Deep Reinforcement Learning is the application of DEEP LEARNING methods to learn the POLICY function.**\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "***\n",
    "\n",
    "#### The Reinforcement Learning Setup \n",
    "\n",
    "The general reinforcement learning setup looks like this: \n",
    "\n",
    "<img src=\"https://www.novatec-gmbh.de/wp-content/uploads/1_mPGk9WTNNvp3i4-9JFgD3w.png\" width=\"500\"/>\n",
    "\n",
    "We can distill the 5 critical parts:\n",
    "\n",
    "* **environment**\n",
    "* **agent**\n",
    "* **reward**\n",
    "* **state**\n",
    "* **action**\n",
    "\n",
    "**THE ENVIRONMENT**\n",
    "\n",
    "The environment can be of various characteristics:\n",
    "\n",
    "Game Environments:\n",
    "\n",
    "<img src=\"https://cdn-media-1.freecodecamp.org/images/1*0gd5LIk1e7RWF3HygxgH-g.png\" width=\"500\"/>\n",
    "\n",
    "OpenAI Gym:\n",
    "\n",
    "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQdjyYQ0psdjCL9oo_ssO482kbWrsJnQ2fPf4LQzgbHGi-T95pxnG6774s3EWTv8ovsggU&usqp=CAU\" width=\"500\"/>\n",
    "\n",
    "Real World:\n",
    "\n",
    "<img src=\"https://events.imeche.org/EventMedia/Images/ProductTitles/44e67e980b314387aad3f5115d9cc627.jpg\" width=\"500\"/>\n",
    "\n",
    "Real World Interaction:\n",
    "\n",
    "<img src=\"https://gumlet.assettype.com/nationalherald%2F2019-11%2F993f2238-a8bb-418b-aa12-c831316ba2db%2Frobot.png?rect=0%2C23%2C2214%2C1245&auto=format%2Ccompress&w=1200\" width=\"500\"/>\n",
    "\n",
    "<br> \n",
    "\n",
    "* STATE\n",
    "\n",
    "The state of the environment is the perfect and complete information of the current environment configuration. \n",
    "\n",
    "<img src=\"https://www.allaboutcircuits.com/uploads/articles/state-table.jpg\" width=\"500\"/>\n",
    "\n",
    "* STATE SPACE\n",
    "\n",
    "The state space is the set of all possible configurations of the environment. \n",
    "\n",
    "<img src=\"https://eliiza.com.au/wp-content/uploads/2018/05/chess-game-tree.jpg\" width=\"500\"/>\n",
    "\n",
    "<img src=\"https://static.wixstatic.com/media/22ed14_2c663d2f34744841a938b0a3378f9e7e~mv2_d_4608_2592_s_4_2.jpg/v1/fill/w_1000,h_563,al_c,q_90,usm_0.66_1.00_0.01/22ed14_2c663d2f34744841a938b0a3378f9e7e~mv2_d_4608_2592_s_4_2.jpg\" width=\"500\"/>\n",
    "\n",
    "<br>\n",
    "\n",
    "* TRANSITION FUNCTION\n",
    "\n",
    "A state transition is the change from the current state to the next state which is described by a **transition function**. State transitions can be deterministic, like in games like \n",
    "chess or non-deterministic, like in dice games. The state transition function takes the current state as input and produces the next state as output. \n",
    "\n",
    "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRBeECOeLEw3HaFWUUwTfp3cprVlYwJ09nr-w&usqp=CAU\" width=\"500\"/>\n",
    "\n",
    "*MECHANICS of the environment:*\n",
    "\n",
    "The setting can be **turn-based**, like e.g. chess or go, **episodic** like e.g. in super-mario or **continous** like e.g. in robotics. \n",
    "\n",
    "*CHARACTERISTICS of the environment:*\n",
    "\n",
    "Fully observable environment, partially observable environment. Deterministic environment, probabilistic/non-deterministic environment. \n",
    "\n",
    "*EPISODES:*\n",
    "\n",
    "An **EPISODE** is a sequence of state-action-reward-state tuples (s,a,s',r'). THIS IS THE LEARNING DATA, WE USE TO BUILD OUR POLICIES FROM!\n",
    "\n",
    "<img src=\"https://github.com/sudharsan13296/Deep-Reinforcement-Learning-With-Python/raw/c2a179457f960f166b8c3395e056827d6d0120e5/01.%20Fundamentals%20of%20Reinforcement%20Learning/Images/21.png\" width=\"500\"/>\n",
    "\n",
    "Episode 1:\n",
    "\n",
    "<img src=\"https://github.com/sudharsan13296/Deep-Reinforcement-Learning-With-Python/raw/c2a179457f960f166b8c3395e056827d6d0120e5/01.%20Fundamentals%20of%20Reinforcement%20Learning/Images/22.png\" width=\"500\"/>\n",
    "\n",
    "Episode 2:\n",
    "\n",
    "<img src=\"https://github.com/sudharsan13296/Deep-Reinforcement-Learning-With-Python/raw/c2a179457f960f166b8c3395e056827d6d0120e5/01.%20Fundamentals%20of%20Reinforcement%20Learning/Images/23.png\" width=\"500\"/>\n",
    "\n",
    "* ...\n",
    "\n",
    "Episode n:\n",
    "\n",
    "<img src=\"https://github.com/sudharsan13296/Deep-Reinforcement-Learning-With-Python/raw/c2a179457f960f166b8c3395e056827d6d0120e5/01.%20Fundamentals%20of%20Reinforcement%20Learning/Images/24.png\" width=\"500\"/>\n",
    "\n",
    "*HORIZON:*\n",
    "\n",
    "Horizon is the time step until which the agent interacts with the environment. We can classify the horizon into **finite horizon** (= chess, agent-environment interaction reaches a terminal state) and **infinite horizon**(= live, continous task, does not have any terminal states). \n",
    "\n",
    "<br>\n",
    "\n",
    "* REWARD FUNCTION\n",
    "\n",
    "The reward is the FEEDBACK that is returned when entering a certain state of the environment. Negative reward is **punishment**. \n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1200/1*z7knMsS6z8DF23DmeU0ZhQ.gif\" width=\"500\"/>\n",
    "\n",
    "* RETURN \n",
    "\n",
    "The return (with respect to the current policy) is the sum of all rewards from the current states to the goal state following a certain policy.  \n",
    "\n",
    "<img src=\"https://github.com/sudharsan13296/Deep-Reinforcement-Learning-With-Python/raw/c2a179457f960f166b8c3395e056827d6d0120e5/01.%20Fundamentals%20of%20Reinforcement%20Learning/Images/25.PNG\" width=\"500\"/>\n",
    "\n",
    "<img src=\"https://static.packt-cdn.com/products/9781789132212/graphics/5afa2f1b-2dd7-47f6-86dd-b4f4a1e09247.png\" width=\"300\"/>\n",
    "\n",
    "*DISCOUNT FACTOR:*\n",
    "\n",
    "The discount factor determines how much we value far future rewards from close future rewards. \n",
    "\n",
    "<img src=\"https://drek4537l1klr.cloudfront.net/morales/v-14/Figures/02_18.png\" width=\"500\"/>\n",
    "\n",
    "<br>\n",
    "\n",
    "* MARKOV PROCESS\n",
    "\n",
    "A Markov process (MP) is a memoryless random process, that means a sequence of random states with the Markov property. A Markov process contains a state space and state transition probability matrix.. A Markov process/Markov chain is then the Tuple of this. A sample in a Markov process is called an episode which ends in a finite state. The state transition matrix fully describes the process, so once we have this matrix we can sample from it. Markov Models describe random processes holding the Markov property.\n",
    "\n",
    "State Transition Table:\n",
    "\n",
    "<img src=\"https://github.com/sudharsan13296/Deep-Reinforcement-Learning-With-Python/raw/c2a179457f960f166b8c3395e056827d6d0120e5/01.%20Fundamentals%20of%20Reinforcement%20Learning/Images/8.PNG\" width=\"500\"/>\n",
    "\n",
    "State Transition Graph:\n",
    "\n",
    "<img src=\"https://github.com/sudharsan13296/Deep-Reinforcement-Learning-With-Python/raw/c2a179457f960f166b8c3395e056827d6d0120e5/01.%20Fundamentals%20of%20Reinforcement%20Learning/Images/9.png\" width=\"500\"/>\n",
    "\n",
    "State Transition Matrix:\n",
    "\n",
    "<img src=\"https://github.com/sudharsan13296/Deep-Reinforcement-Learning-With-Python/raw/c2a179457f960f166b8c3395e056827d6d0120e5/01.%20Fundamentals%20of%20Reinforcement%20Learning/Images/10.PNG\" width=\"500\"/>\n",
    "\n",
    "* MARKOV DECISION PROCESS\n",
    "\n",
    "A markov decision process (MDP) is an extension of a  markov reward process (MRP) (=markov process with reward functions for states) is a Markov process/chain with state values, that means we have a state space, a transition probability matrix, a reward function that gives us the expected reward of the next state given the current state and a discount factor which describes if we are more interested in rewards reachable in the closer next state transitions or farther rewards in states reachable after a lot transitions. The discount rate is the present value of future rewards in the current state, that means how much I care now about the rewards of the future. We need a discount rate because we almost every time  do not know the future/world completely so we have to take in some reward now to get along. If all sequences in our Markov process terminate, that means that they are all episodes, it is possible to not discount the future rewards. A Markov reward process is actually a markov process where the states are valued due to some policy. \n",
    "\n",
    "<img src=\"https://github.com/sudharsan13296/Deep-Reinforcement-Learning-With-Python/raw/c2a179457f960f166b8c3395e056827d6d0120e5/01.%20Fundamentals%20of%20Reinforcement%20Learning/Images/11.png\" width=\"500\"/>\n",
    "\n",
    "<img src=\"https://github.com/sudharsan13296/Deep-Reinforcement-Learning-With-Python/raw/c2a179457f960f166b8c3395e056827d6d0120e5/01.%20Fundamentals%20of%20Reinforcement%20Learning/Images/12.png\" width=\"500\"/>\n",
    "\n",
    "<img src=\"https://github.com/sudharsan13296/Deep-Reinforcement-Learning-With-Python/raw/c2a179457f960f166b8c3395e056827d6d0120e5/01.%20Fundamentals%20of%20Reinforcement%20Learning/Images/13.png\" width=\"500\"/>\n",
    "\n",
    "What comes in new is the reward function:\n",
    "\n",
    "<img src=\"https://github.com/sudharsan13296/Deep-Reinforcement-Learning-With-Python/raw/c2a179457f960f166b8c3395e056827d6d0120e5/01.%20Fundamentals%20of%20Reinforcement%20Learning/Images/14.png\" width=\"500\"/>\n",
    "\n",
    "<img src=\"https://github.com/sudharsan13296/Deep-Reinforcement-Learning-With-Python/raw/c2a179457f960f166b8c3395e056827d6d0120e5/01.%20Fundamentals%20of%20Reinforcement%20Learning/Images/15.png\" width=\"500\"/>\n",
    "\n",
    "\n",
    "*IMPORTANT PROBLEMS:*\n",
    "\n",
    "> Credit Assignment Problem: If i am playing a game with N steps, how should i know which of the N steps were responsible for me being successful in the end? what \n",
    "are the crucial steps that make me succeed? how do i assign the credit of the reward correctly to those deciding steps?\n",
    "\n",
    "> *A Chinese farmer gets a horse, which soon runs away. A neighbor says, “So, sad. That’s bad\n",
    "news.” The farmer replies, “Good news, bad news, who can say?”\n",
    "The horse comes back and brings another horse with him. The neighbor says, “How lucky.\n",
    "That’s good news.” The farmer replies, “Good news, bad news, who can say?”\n",
    "The farmer gives the second horse to his son, who rides it, then is thrown and badly breaks\n",
    "his leg. The neighbor says, “So sorry for your son. This is definitely bad news.” The farmer\n",
    "replies, “Good news, bad news, who can say?”\n",
    "In a week or so, the emperor’s men come and take every healthy young man to fight in a\n",
    "war. The farmer’s son is spared.\n",
    "So, good news or bad news? Who can say?*\n",
    "\n",
    "> Exploration vs. Exploitation Problem: How do we encourage the agent to explore but also exploit? \n",
    "\n",
    "<br>\n",
    "\n",
    "***\n",
    "\n",
    "**THE AGENT**\n",
    "\n",
    "The agent **interacts** with the environment to pursue some **GOAL**.\n",
    "\n",
    "* ACTION\n",
    "\n",
    "An action is a manipulation of the environmnt executed executed by the agent. In RL it is often used similarly to **decision.**\n",
    "\n",
    "* ACTION SPACE\n",
    "\n",
    "The action space is the space of all possible actions that an agent can execute. Action spaces can be **discrete** or **continous.**\n",
    "\n",
    "E.g. [L,R,T,B]\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/782/1*wLUSa716Be2cS3E3t3AjvA.png\" width=\"500\"/>\n",
    "\n",
    "* OBSERVATION\n",
    "\n",
    "The observation or sensing is what the agent perceives of the environment, which could be noisy and/or incomplete.\n",
    "\n",
    "Observations can handcrafted feature encodings:\n",
    "\n",
    "<img src=\"https://qiskit.org/textbook/ch-states/images/car_track_2.jpg\" width=\"500\"/>\n",
    "\n",
    "OR e.g. raw image observations:\n",
    "\n",
    "<img src=\"data:image/png;base64,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\" width=\"500\"/>\n",
    "\n",
    "<img src=\"https://d3i71xaburhd42.cloudfront.net/4c7028640e3470a73af84d22eafa78855931c70f/20-Figure2-1.png\" width=\"500\"/>\n",
    "\n",
    "* OBSERVATION SPACE\n",
    "\n",
    "The observation space is the space of all possible observations. For a 8x8 black-white image this would be 8x8x2=128 possible OBSERVATIONS within the observation space. \n",
    "\n",
    "In code this actually looks like:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/3678/1*uYKTt-1ZupnoSePzYo52Yg.png\" width=\"500\"/>\n",
    "\n",
    "* POLICY\n",
    "\n",
    "A **Policy Function** is a function that takes an **observation** (if the world is fully observable => observation = state) as input and returns an **action**. Policies can have\n",
    "different characteristics (deterministic, non-deterministic). \n",
    "\n",
    "<img src=\"https://github.com/sudharsan13296/Deep-Reinforcement-Learning-With-Python/raw/c2a179457f960f166b8c3395e056827d6d0120e5/01.%20Fundamentals%20of%20Reinforcement%20Learning/Images/17.png\" width=\"500\"/>\n",
    "\n",
    "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQFcGZUcsVlxCtD4MK0djoTbprLUF99EvfSolz4_KajWBNcxdF_RIlSkOj05vHKsFL8ndM&usqp=CAU\" width=\"500\"/>\n",
    "\n",
    "We can think of a an optimal policy as a **gradient field**, where we are hiking along the ridge of the reward function. \n",
    "\n",
    "<br>\n",
    "\n",
    "* VALUE FUNCTION\n",
    "\n",
    "A **Value Function** is a function, that assigns a value to each state-action pair concerning how good the action in that respective state was to reach the overall goal. The simplest value function is a **State - Action - VALUE - TABLE** (often called **Q-TABLE**), that denotes (based on past experience) how good each action in each state was for reaching the overall goal. \n",
    "\n",
    "State Value under a certain policy:\n",
    "\n",
    "<img src=\"https://github.com/sudharsan13296/Deep-Reinforcement-Learning-With-Python/raw/c2a179457f960f166b8c3395e056827d6d0120e5/01.%20Fundamentals%20of%20Reinforcement%20Learning/Images/29.PNG\" width=\"500\"/>\n",
    "\n",
    "<img src=\"https://render.githubusercontent.com/render/math?math=V%28A%29%20%3D%20R%28%5Ctau_1%29%20%3D%201%2B1%2B1%2B1%20%3D4&mode=inline\" width=\"300\"/>\n",
    "\n",
    "\n",
    "State Value following another policy:\n",
    "\n",
    "<img src=\"https://github.com/sudharsan13296/Deep-Reinforcement-Learning-With-Python/raw/c2a179457f960f166b8c3395e056827d6d0120e5/01.%20Fundamentals%20of%20Reinforcement%20Learning/Images/30.PNG\" width=\"500\"/>\n",
    "\n",
    "<img src=\"https://render.githubusercontent.com/render/math?math=V%28A%29%20%3D%20R%28%5Ctau_2%29%20%3D%20-1%2B1%2B1%2B1%20%3D2&mode=inline\" width=\"300\"/>\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/384/1*RhO9Ulh5nF_zc2pcgBHzAg.png\" width=\"500\"/>\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1920/1*f8PhlUiGYVv_FJcgyFNGfw.png\" width=\"500\"/>\n",
    "\n",
    "<img src=\"https://www.datascienceblog.net/post/reinforcement-learning/policy_value_function_optimal_improved.png\" width=\"500\"/>\n",
    "\n",
    "A first, intuitive approache e.g. would be, to always ask the **value-function**, given the current state, what the action with the highest possible reward would be. By **exploration** we try out different actions in different states and update our **Q-TABLE** accordingly. This algorithm is called **VALUE ITERATION**.\n",
    "\n",
    "Here, we can also see a first difference between what is called REINFORCEMENT LEARNING and DEEP REINFORCEMENT LEARNING. In some applications, the extend of the Q-Table is so vast, that\n",
    "that it is not suitable anymore, to keep it in memory. The idea that follows is, to approximate the Q-Table with a Q-Function, which (you guessed it ...) is a **deep neural network**.\n",
    "\n",
    "<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2019/04/Screenshot-2019-04-16-at-5.46.01-PM.png\" width=\"500\"/>\n",
    "\n",
    "In conclusion:\n",
    "\n",
    "<img src=\"https://github.com/sudharsan13296/Deep-Reinforcement-Learning-With-Python/raw/c2a179457f960f166b8c3395e056827d6d0120e5/01.%20Fundamentals%20of%20Reinforcement%20Learning/Images/32.PNG\" width=\"500\"/>\n",
    "\n",
    "<img src=\"https://github.com/sudharsan13296/Deep-Reinforcement-Learning-With-Python/raw/c2a179457f960f166b8c3395e056827d6d0120e5/01.%20Fundamentals%20of%20Reinforcement%20Learning/Images/33.PNG\" width=\"500\"/>\n",
    "\n",
    "<img src=\"https://github.com/sudharsan13296/Deep-Reinforcement-Learning-With-Python/raw/c2a179457f960f166b8c3395e056827d6d0120e5/01.%20Fundamentals%20of%20Reinforcement%20Learning/Images/34.PNG\" width=\"500\"/>\n",
    "\n",
    "> In continous action spaces, which are the case in most of the real world applications, first estimating the value-function and deriving the optimal policy from that gets more and more difficult because of the vast action spaces. Another approach here, is then to build up the **policy function** directly given current state and producing the next action. To achieve this, a reward function has to be defined, towards which we optimize. We train our **policy network** with the goal to maximize the expected return. This method is called **policy gradients**. \n",
    "\n",
    "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTTw_55qgJ0Br1a4o9618tKhm4Wlo7mhnARhw&usqp=CAU\" width=\"500\"/>\n",
    "\n",
    "*\n",
    "\n",
    "The way we build our policy determines different kinds of reinforcement learning:\n",
    "\n",
    "*Model Based Reinforcement Learning:* The idea in Model-Based-RL is to build up a model of the environment and then use this model as a base for **SEARCHING/PLANNING** towards the goal. Methods applied in this field are *dynamic programming*(DIVIDE & CONQUER) or *monte-carlo-methods*(INTELLIGENT SAMPLING). This method implies the extraction of important **concepts** of the environment (e.g. opponents, positions, states) and the **transitions** (= mechanics/physics of the environment, e.g. is it turn-based, how to move, act, ...). \n",
    "\n",
    "<img src=\"https://lilianweng.github.io/lil-log/assets/images/TD_MC_DP_backups.png\" width=\"500\"/>\n",
    "\n",
    "*Model-Free Reinforcement Learning*: The idea in Model-Free RL is to learn how to act optimally based on past **experience**/interactions with the environment and derive an optimal policy from that without building a model of the world. (INTUITION). \n",
    "\n",
    "*On-Policy Learning:* Learn from state-action-reward sequences that have been produced by the current active policy. \n",
    "\n",
    "*Off-Policy RL:* Learn from state-action-reward sequences that have not been produced by the current active policy. \n",
    "\n",
    "\n",
    "```\n",
    "QUESTION TO YOU: Who thinks that self-driving cars can be trained following this RL framework?  \n",
    "\n",
    "A: yes, \n",
    "\n",
    "B: no, \n",
    "```\n",
    "\n",
    "If yes: here's a paper and a blogpost for you: https://www.semanticscholar.org/paper/Mastering-Atari%2C-Go%2C-Chess-and-Shogi-by-Planning-a-Schrittwieser-Antonoglou/3507bd62a14bd0e8ead28cdedb1c33ba83c39c6b and https://mixed.de/elon-musk-tesla-autopilot-bald-deutlich-sicherer-als-der-mensch/\n",
    "\n",
    "```\n",
    "QUESTION TO YOU: Who thinks that dialogue can be trained following this RL framework?  \n",
    "\n",
    "A: yes, \n",
    "\n",
    "B: no, \n",
    "```\n",
    "\n",
    "If yes, welcome to the practice part ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b59c56",
   "metadata": {},
   "source": [
    "## C) Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3764d0b",
   "metadata": {},
   "source": [
    "#### Reinforcement Learning I\n",
    "\n",
    "Today: setting up a reinforcement learning experiment:\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0afdb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Check GPU reachability \n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da18fbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "from abc import ABC, abstractmethod\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92430bd",
   "metadata": {},
   "source": [
    "#### The Environment\n",
    "\n",
    "I use my own implementation of an environment based on the **OpenAI Gym Environment**. \n",
    "\n",
    "You can find the openai gym for your experiments here: https://github.com/openai/gym#examples \n",
    "\n",
    "A nice introduction on how to setup openai gym can be found here: https://www.youtube.com/watch?v=8MC3y7ASoPs and here: https://gym.openai.com/docs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ee15e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOMAIN = the data_object (JSON SERIALIZABLE)\n",
    "class Environment():\n",
    "    \"\"\"The main Environment class. It encapsulates an environment with\n",
    "    arbitrary behind-the-scenes dynamics. An environment can be\n",
    "    partially or fully observed.\n",
    "    The main API methods that users of this class need to know are:\n",
    "        step\n",
    "        reset\n",
    "        render\n",
    "        close\n",
    "        seed\n",
    "    And set the following attributes:\n",
    "        action_space: The Space object corresponding to valid actions\n",
    "        observation_space: The Space object corresponding to valid observations\n",
    "        reward_range: A tuple corresponding to the min and max possible rewards\n",
    "    Note: a default reward range set to [-inf,+inf] already exists. Set it if you want a narrower range.\n",
    "    The methods are accessed publicly as \"step\", \"reset\", etc...\n",
    "    \"\"\"\n",
    "    # Set this in SOME subclasses\n",
    "    metadata = {'render.modes': []}\n",
    "    reward_range = (-float('inf'), float('inf'))\n",
    "    spec = None\n",
    "\n",
    "    def __init__(self, action_space=None, observation_space=None):\n",
    "        # Set variables\n",
    "        self.action_space = action_space\n",
    "        self.observation_space = observation_space\n",
    "    \n",
    "# REPOSITORY = the functionality interface\n",
    "class EnvironmentRepository(ABC):\n",
    "    @abstractmethod\n",
    "    def step(self, action):\n",
    "        \"\"\"Run one timestep of the environment's dynamics. When end of\n",
    "        episode is reached, you are responsible for calling `reset()`\n",
    "        to reset this environment's state.\n",
    "        Accepts an action and returns a tuple (observation, reward, done, info).\n",
    "        Args:\n",
    "            action (object): an action provided by the agent\n",
    "        Returns:\n",
    "            observation (object): agent's observation of the current environment\n",
    "            reward (float) : amount of reward returned after previous action\n",
    "            done (bool): whether the episode has ended, in which case further step() calls will return undefined results\n",
    "            info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the environment to an initial state and returns an initial\n",
    "        observation.\n",
    "        Note that this function should not reset the environment's random\n",
    "        number generator(s); random variables in the environment's state should\n",
    "        be sampled independently between multiple calls to `reset()`. In other\n",
    "        words, each call of `reset()` should yield an environment suitable for\n",
    "        a new episode, independent of previous episodes.\n",
    "        Returns:\n",
    "            observation (object): the initial observation.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"Renders the environment.\n",
    "        The set of supported modes varies per environment. (And some\n",
    "        environments do not support rendering at all.) By convention,\n",
    "        if mode is:\n",
    "        - human: render to the current display or terminal and\n",
    "          return nothing. Usually for human consumption.\n",
    "        - rgb_array: Return an numpy.ndarray with shape (x, y, 3),\n",
    "          representing RGB values for an x-by-y pixel image, suitable\n",
    "          for turning into a video.\n",
    "        - ansi: Return a string (str) or StringIO.StringIO containing a\n",
    "          terminal-style text representation. The text can include newlines\n",
    "          and ANSI escape sequences (e.g. for colors).\n",
    "        Note:\n",
    "            Make sure that your class's metadata 'render.modes' key includes\n",
    "              the list of supported modes. It's recommended to call super()\n",
    "              in implementations to use the functionality of this method.\n",
    "        Args:\n",
    "            mode (str): the mode to render with\n",
    "        Example:\n",
    "        class MyEnv(Env):\n",
    "            metadata = {'render.modes': ['human', 'rgb_array']}\n",
    "            def render(self, mode='human'):\n",
    "                if mode == 'rgb_array':\n",
    "                    return np.array(...) # return RGB frame suitable for video\n",
    "                elif mode == 'human':\n",
    "                    ... # pop up a window and render\n",
    "                else:\n",
    "                    # just raise an exception\n",
    "                    super(MyEnv, self).render(mode=mode)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def close(self):\n",
    "        \"\"\"Override close in your subclass to perform any necessary cleanup.\n",
    "        Environments will automatically close() themselves when\n",
    "        garbage collected or when the program exits.\n",
    "        \"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66864695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPOSITORY IMPLEMENTATION = the way how you would like to implement it\n",
    "class EnvironmentRepositoryImpl(EnvironmentRepository):\n",
    "    # Initialize / Instance Attributes\n",
    "    def __init__(self, environment):\n",
    "        # Set variables\n",
    "        self.data_object = environment\n",
    "        print('Environment initialized')\n",
    "\n",
    "    def step(self, action):\n",
    "        print('ENV: step executed.')\n",
    "        print('ENV: action received {0}'.format(action))\n",
    "        observation = \"\"\n",
    "        reward = \"\"\n",
    "        done = False\n",
    "        info = {\"test\": \"\"}\n",
    "        state = observation, reward, done, info\n",
    "        return state\n",
    "\n",
    "    def reset(self):\n",
    "        print('...environment reset executed.')\n",
    "        observation = \"\"\n",
    "        reward = \"\"\n",
    "        done = False\n",
    "        info = {\"test\": \"\"}\n",
    "        state = observation, reward, done, info\n",
    "        return state\n",
    "\n",
    "    def render(self):\n",
    "        print('environment render executed.')\n",
    "\n",
    "    def close(self):\n",
    "        print('environment close executed.')\n",
    "\n",
    "    def get_action_space(self):\n",
    "        # get action space from api of the playground or via js in browser using selenium\n",
    "        print('...returning action space.')\n",
    "        action_space = \"\"\n",
    "        return action_space\n",
    "\n",
    "    def get_observation_space(self):\n",
    "        # get observation space of the playground from api or via js in browser using selenium\n",
    "        print('...returning observation space.')\n",
    "        observation_space = \"\"\n",
    "        return observation_space\n",
    "\n",
    "    def get_reward_range(self):\n",
    "        # get reward range of the playground from api or via js in browser using selenium\n",
    "        print('...returning reward range.')\n",
    "        reward_range = \"\"\n",
    "        return reward_range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259b3143",
   "metadata": {},
   "source": [
    "#### The Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "825577e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOMAIN\n",
    "class Agent():\n",
    "    # class variables\n",
    "    agent_variable = \"\"\n",
    "    # class methods\n",
    "    def __init__(self, agent_variable=\"\"):\n",
    "        self.agent_variable = agent_variable\n",
    "        \n",
    "# REPOSITORY\n",
    "class AgentRepository(ABC):\n",
    "    @abstractmethod\n",
    "    def get_action(self, state): # This is the agent's POLICY, if you will\n",
    "        \"\"\" Agent gets a state as input and returns an action \n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bb431e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPOSITORY IMPLEMENTATION\n",
    "class AgentRepositoryImpl(AgentRepository):\n",
    "    # Initializer / Instance Attributes\n",
    "    def __init__(self, environment, agent):\n",
    "        # Set variables\n",
    "        self.data_object = agent\n",
    "        self.environment = environment\n",
    "        self.action_space = self.environment.action_space\n",
    "        print('CartpoleAgent initialized.')\n",
    "\n",
    "    def get_action(self, state):\n",
    "        # evaluate state here\n",
    "        print('AGENT: state received {0}'.format(state))\n",
    "        # get action\n",
    "        action_set = random.sample(self.action_space, 1)\n",
    "        action = action_set[0]\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c1e9d9",
   "metadata": {},
   "source": [
    "#### The Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d4c76e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment initialized\n",
      "CartpoleAgent initialized.\n"
     ]
    }
   ],
   "source": [
    "# Setting up the Environment\n",
    "env_action_space = [0,1]\n",
    "env_observation_space = [1,2,3,4,5,6]\n",
    "\n",
    "environment_data_object = Environment(env_action_space, env_observation_space)\n",
    "\n",
    "environment = EnvironmentRepositoryImpl(environment_data_object)\n",
    "\n",
    "# Setting up the Agent\n",
    "agent_data_object = Agent()\n",
    "agent = AgentRepositoryImpl(environment_data_object, agent_data_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef4ac86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...training loop started.\n",
      "...environment reset executed.\n",
      "AGENT: state received ('', '', False, {'test': ''})\n",
      "ENV: step executed.\n",
      "ENV: action received 0\n",
      "AGENT: state received ('', '', False, {'test': ''})\n",
      "ENV: step executed.\n",
      "ENV: action received 1\n",
      "AGENT: state received ('', '', False, {'test': ''})\n",
      "ENV: step executed.\n",
      "ENV: action received 1\n",
      "AGENT: state received ('', '', False, {'test': ''})\n",
      "ENV: step executed.\n",
      "ENV: action received 1\n",
      "AGENT: state received ('', '', False, {'test': ''})\n",
      "ENV: step executed.\n",
      "ENV: action received 0\n",
      "AGENT: state received ('', '', False, {'test': ''})\n",
      "ENV: step executed.\n",
      "ENV: action received 0\n",
      "AGENT: state received ('', '', False, {'test': ''})\n",
      "ENV: step executed.\n",
      "ENV: action received 0\n",
      "AGENT: state received ('', '', False, {'test': ''})\n",
      "ENV: step executed.\n",
      "ENV: action received 0\n",
      "AGENT: state received ('', '', False, {'test': ''})\n",
      "ENV: step executed.\n",
      "ENV: action received 0\n",
      "AGENT: state received ('', '', False, {'test': ''})\n",
      "ENV: step executed.\n",
      "ENV: action received 0\n",
      "environment close executed.\n",
      "DONE: training loop executed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Starting the training\n",
    "print(\"...training loop started.\")\n",
    "\n",
    "# before running the episode, reset the environment once \n",
    "state = environment.reset()\n",
    "\n",
    "# Run episode of 100 steps\n",
    "for _ in range(10):\n",
    "    # take the state from the environment, give it to the internal policy and return an action\n",
    "    action = agent.get_action(state)\n",
    "    # give the action to the environment and get the next state\n",
    "    #observation, reward, done, info = environment.step(\n",
    "    #    action)\n",
    "    state = environment.step(action)\n",
    "    # OPTIONAL: if the environment is visualizable (e.g. tic tac toe, etc.) visualize it\n",
    "    #environment.render()\n",
    "    # ONLY FOR EXPLANATION REASONS ...\n",
    "    time.sleep(2)\n",
    "\n",
    "environment.close()\n",
    "print('DONE: training loop executed successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc400f20",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96214302",
   "metadata": {},
   "source": [
    "# TODO's\n",
    "\n",
    "1. Choose a date for your paper discussion/presentation by **Friday, 07.05.2021 12.00 PM/noon** at https://terminplaner4.dfn.de/XP1MLFhVyz1eA44a\n",
    "\n",
    "2. Send your finished presentations (+ possibly annotated paper) by **Monday 12.00 AM/midnight** via email to henrik.voigt@uni-jena.de\n",
    "\n",
    "3. SEND your little HOMEWORK to henrik.voigt@uni-jena.de by using the naming convention: HOMEWORK_01_FIRSTNAME_LASTNAME.ipynb until May 19th 00.00 AM/midnight\n",
    "\n",
    "***\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env-kernel",
   "language": "python",
   "name": "pytorch-env-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
